{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- PM2-5: string (nullable = true)\n",
      " |-- PM10: string (nullable = true)\n",
      " |-- PM1-0: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- BC: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import col, when\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(\"C:/Users/Winsido/Desktop/VGP-week3-data.csv\")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    (\"PM2-5\"),\n",
    "    when(\n",
    "        col(\"PM2-5\") == \"NULL\",\n",
    "        None\n",
    "    ).otherwise(col(\"PM2-5\"))\n",
    ")\n",
    "df = df.withColumn(\n",
    "    (\"PM10\"),\n",
    "    when(\n",
    "        col(\"PM10\") == \"NULL\",\n",
    "        None\n",
    "    ).otherwise(col(\"PM10\"))\n",
    ")\n",
    "df = df.withColumn(\n",
    "    (\"PM1-0\"),\n",
    "    when(\n",
    "        col(\"PM1-0\") == \"NULL\",\n",
    "        None\n",
    "    ).otherwise(col(\"PM1-0\"))\n",
    ")\n",
    "df = df.withColumn(\n",
    "    (\"NO2\"),\n",
    "    when(\n",
    "        col(\"NO2\") == \"NULL\",\n",
    "        None\n",
    "    ).otherwise(col(\"NO2\"))\n",
    ")\n",
    "df = df.withColumn(\n",
    "    (\"activity\"),\n",
    "    when(\n",
    "        col(\"activity\") == \"NULL\",\n",
    "        None\n",
    "    ).otherwise(col(\"activity\"))\n",
    ")\n",
    "df = df.withColumn(\n",
    "    (\"event\"),\n",
    "    when(\n",
    "        col(\"event\") == \"NULL\",\n",
    "        None\n",
    "    ).otherwise(col(\"event\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+----------------+----------------+-----+----+-----+----+----+--------+-----+\n",
      "|kit_id|participant_id|                time|             lat|             lon|PM2-5|PM10|PM1-0| NO2|  BC|activity|event|\n",
      "+------+--------------+--------------------+----------------+----------------+-----+----+-----+----+----+--------+-----+\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717466666667|2.00590833333333| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:03:...|48.7717466666667|2.00590833333333| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:03:...|       48.771765|2.00590333333333| null|null| null|null|null|    null| null|\n",
      "+------+--------------+--------------------+----------------+----------------+-----+----+-----+----+----+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+----------------+----------------+-----+----+-----+---+---+--------+-------------------+\n",
      "|kit_id|participant_id|                time|             lat|             lon|PM2-5|PM10|PM1-0|NO2| BC|activity|              event|\n",
      "+------+--------------+--------------------+----------------+----------------+-----+----+-----+---+---+--------+-------------------+\n",
      "|    80|       9999964|2019-11-15 09:00:...|48.7717183333333|2.00601666666667| null|null| null|  7|369|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:00:...|48.7717183333333|2.00601666666667| null|null| null|  7|369|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:00:...|48.7717183333333|2.00601666666667| null|null| null|  7|369|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:00:...|48.7717216666667|2.00587833333333| null|null| null|  7|369|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:00:...|48.7717266666667|2.00586333333333| null|null| null|  7|369|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:00:...|48.7717266666667|2.00586333333333| null|null| null|  7|369|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:01:...|48.7717266666667|2.00586333333333| null|null| null|  6|396|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:01:...|48.7717266666667|2.00586333333333| null|null| null|  6|396|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:01:...|48.7717266666667|2.00586333333333| null|null| null|  6|396|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:01:...|48.7717266666667|2.00586333333333| null|null| null|  6|396|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:01:...|48.7717266666667|2.00586333333333| null|null| null|  6|396|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:01:...|48.7717266666667|2.00586333333333| null|null| null|  6|396|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:02:...|48.7717266666667|2.00586333333333| null|null| null|  6|382|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:02:...|48.7717266666667|2.00586333333333| null|null| null|  6|382|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:02:...|48.7717266666667|2.00586333333333| null|null| null|  6|382|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:02:...|48.7717266666667|2.00586333333333| null|null| null|  6|382|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:02:...|48.7717266666667|2.00586333333333| null|null| null|  6|382|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:02:...|48.7717266666667|2.00586333333333| null|null| null|  6|382|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:03:...|48.7717266666667|2.00586333333333|    3|   4|    4|  7|376|  Bureau|Arrêter De Cuisiner|\n",
      "|    80|       9999964|2019-11-15 09:03:...|48.7717266666667|2.00586333333333|    3|   4|    4|  7|376|  Bureau|Arrêter De Cuisiner|\n",
      "+------+--------------+--------------------+----------------+----------------+-----+----+-----+---+---+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"activity\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+----------------+--------+-----+----+-----+----+----+--------+-----+\n",
      "|kit_id|participant_id|                time|             lat|     lon|PM2-5|PM10|PM1-0| NO2|  BC|activity|event|\n",
      "+------+--------------+--------------------+----------------+--------+-----+----+-----+----+----+--------+-----+\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|2.006005| null|null| null|null|null|    null| null|\n",
      "+------+--------------+--------------------+----------------+--------+-----+----+-----+----+----+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.sort(\"time\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- PM2-5: string (nullable = true)\n",
      " |-- PM10: string (nullable = true)\n",
      " |-- PM1-0: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- BC: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- PM2-5: double (nullable = true)\n",
      " |-- PM10: string (nullable = true)\n",
      " |-- PM1-0: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- BC: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o428.showString.\n: java.io.IOException: Failed to create local dir in C:\\Users\\Winsido\\AppData\\Local\\Temp\\blockmgr-53dcab76-c224-4d90-aabb-f6261c2b45d0\\34.\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:73)\r\n\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:137)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:690)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1729)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1260)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:272)\r\n\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1196)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$writeBlocks$1(TorrentBroadcast.scala:147)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$writeBlocks$1$adapted(TorrentBroadcast.scala:141)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:141)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:91)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:35)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:77)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1480)\r\n\tat org.apache.spark.sql.execution.datasources.v2.csv.CSVScan.createReaderFactory(CSVScan.scala:83)\r\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.readerFactory$lzycompute(BatchScanExec.scala:45)\r\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.readerFactory(BatchScanExec.scala:45)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:61)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:60)\r\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.supportsColumnar(BatchScanExec.scala:29)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.insertTransitions(Columnar.scala:501)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.$anonfun$insertTransitions$1(Columnar.scala:506)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.immutable.List.map(List.scala:298)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.insertTransitions(Columnar.scala:506)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.$anonfun$insertTransitions$1(Columnar.scala:506)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.immutable.List.map(List.scala:298)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.insertTransitions(Columnar.scala:506)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.$anonfun$insertTransitions$1(Columnar.scala:506)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.immutable.List.map(List.scala:298)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.insertTransitions(Columnar.scala:506)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.apply(Columnar.scala:514)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.apply(Columnar.scala:481)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$prepareForExecution$1(QueryExecution.scala:117)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\r\n\tat org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:117)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:96)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:96)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$writePlans$5(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$.append(QueryPlan.scala:359)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$writePlans(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toString(QueryExecution.scala:174)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3403)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2516)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2723)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:297)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:334)\r\n\tat sun.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-3b788c2380a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"time\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTimestampType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"PM2-5\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    360\u001b[0m         \"\"\"\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o428.showString.\n: java.io.IOException: Failed to create local dir in C:\\Users\\Winsido\\AppData\\Local\\Temp\\blockmgr-53dcab76-c224-4d90-aabb-f6261c2b45d0\\34.\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:73)\r\n\tat org.apache.spark.storage.DiskStore.contains(DiskStore.scala:137)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:690)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1729)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1260)\r\n\tat org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:272)\r\n\tat org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:1196)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$writeBlocks$1(TorrentBroadcast.scala:147)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$writeBlocks$1$adapted(TorrentBroadcast.scala:141)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:141)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:91)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:35)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:77)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1480)\r\n\tat org.apache.spark.sql.execution.datasources.v2.csv.CSVScan.createReaderFactory(CSVScan.scala:83)\r\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.readerFactory$lzycompute(BatchScanExec.scala:45)\r\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.readerFactory(BatchScanExec.scala:45)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:61)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:60)\r\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.supportsColumnar(BatchScanExec.scala:29)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.insertTransitions(Columnar.scala:501)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.$anonfun$insertTransitions$1(Columnar.scala:506)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.immutable.List.map(List.scala:298)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.insertTransitions(Columnar.scala:506)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.$anonfun$insertTransitions$1(Columnar.scala:506)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.immutable.List.map(List.scala:298)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.insertTransitions(Columnar.scala:506)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.$anonfun$insertTransitions$1(Columnar.scala:506)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.immutable.List.map(List.scala:298)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.insertTransitions(Columnar.scala:506)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.apply(Columnar.scala:514)\r\n\tat org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions.apply(Columnar.scala:481)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$prepareForExecution$1(QueryExecution.scala:117)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\r\n\tat org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:117)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:96)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:96)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$writePlans$5(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$.append(QueryPlan.scala:359)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$writePlans(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toString(QueryExecution.scala:174)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3403)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2516)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2723)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:297)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:334)\r\n\tat sun.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType, DateType,TimestampType\n",
    "df = df.withColumn(\"PM2-5\",df[\"PM2-5\"].cast(DoubleType()))\n",
    "df = df.withColumn(\"time\",df[\"time\"].cast(TimestampType()))\n",
    "df.printSchema()\n",
    "df.filter(df[\"PM2-5\"] > 60).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+----------------+----------------+-----+----+-----+----+----+--------+-----+\n",
      "|kit_id|participant_id|                time|             lat|             lon|PM2-5|PM10|PM1-0| NO2|  BC|activity|event|\n",
      "+------+--------------+--------------------+----------------+----------------+-----+----+-----+----+----+--------+-----+\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717466666667|2.00590833333333| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:03:...|48.7717466666667|2.00590833333333| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|2019-11-14 09:03:...|       48.771765|2.00590333333333| null|null| null|null|null|    null| null|\n",
      "+------+--------------+--------------------+----------------+----------------+-----+----+-----+----+----+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- PM2-5: double (nullable = true)\n",
      " |-- PM10: string (nullable = true)\n",
      " |-- PM1-0: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- BC: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+----------------+----------------+-----+----+-----+----+----+--------+-----+-------+\n",
      "|kit_id|participant_id|                time|             lat|             lon|PM2-5|PM10|PM1-0| NO2|  BC|activity|event|timefaz|\n",
      "+------+--------------+--------------------+----------------+----------------+-----+----+-----+----+----+--------+-----+-------+\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:00:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:01:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:02:...|48.7717466666667|2.00590833333333| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:03:...|48.7717466666667|2.00590833333333| null|null| null|null|null|    null| null|   null|\n",
      "|    80|       9999964|2019-11-14 09:03:...|       48.771765|2.00590333333333| null|null| null|null|null|    null| null|   null|\n",
      "+------+--------------+--------------------+----------------+----------------+-----+----+-----+----+----+--------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#On essaye de convertir en timestamp\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "new_df = df.withColumn('timefaz',to_timestamp(col('time'), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
