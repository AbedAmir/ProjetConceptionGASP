{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, to_timestamp, unix_timestamp\n",
    "from pyspark.sql.types import DoubleType\n",
    "from  pyspark.sql.functions import abs, pow\n",
    "import pyspark.sql.functions as F\n",
    "import time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = SparkContext.getOrCreate()\n",
    "# spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField,StructType, BooleanType, DoubleType,LongType, IntegerType)\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : SPOINT\n",
    "\n",
    "spoint_schema = [StructField('lat', FloatType(),True),\n",
    "                 StructField('lon', FloatType(),True)]\n",
    "spoint = StructType(fields=spoint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : SECTION\n",
    "\n",
    "section_schema = [StructField('lat1', FloatType(),True),\n",
    "                  StructField('lon1', FloatType(),True),\n",
    "                  StructField('lat2', FloatType(),True),\n",
    "                  StructField('lon2', FloatType(),True)]\n",
    "section = StructType(fields=section_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : SLINE\n",
    "\n",
    "sline_schema = [StructField('rints', ArrayType(section),True)]\n",
    "sline = StructType(fields=sline_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USPOINT\n",
    "\n",
    "uspoint_schema = [StructField('t1', LongType(),True),\n",
    "                  StructField('t2', LongType(),True),\n",
    "                  StructField('lat1', FloatType(),True),\n",
    "                  StructField('lon1', FloatType(),True),\n",
    "                  StructField('lat2', FloatType(),True),\n",
    "                  StructField('lon2', FloatType(),True)]\n",
    "uspoint = StructType(fields=uspoint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSPOINT\n",
    "\n",
    "mspoint_schema = [StructField('rints', ArrayType(uspoint),True)]\n",
    "mspoint = StructType(fields=mspoint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : UINT\n",
    "\n",
    "uint_schema = [StructField('val', IntegerType(),True),\n",
    "               StructField('t1', LongType(),True),\n",
    "               StructField('t2', LongType(),True)]\n",
    "uint = StructType(fields=uint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MINT\n",
    "\n",
    "mint_schema = [StructField('units', ArrayType(uint),True)]\n",
    "mint = StructType(fields=mint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USTRING\n",
    "\n",
    "ustring_schema = [StructField('val', StringType(),True),\n",
    "               StructField('t1', LongType(),True),\n",
    "               StructField('t2', LongType(),True)]\n",
    "ustring = StructType(fields=ustring_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSTRING\n",
    "\n",
    "mstring_schema = [StructField('units', ArrayType(ustring),True)]\n",
    "mstring = StructType(fields=mstring_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : UREAL\n",
    "\n",
    "ureal_schema = [StructField('a', FloatType(),True),\n",
    "              StructField('b', FloatType(),True),\n",
    "              StructField('c', FloatType(),True),\n",
    "              StructField('r', BooleanType(),True),\n",
    "              StructField('t1', LongType(),True),\n",
    "              StructField('t2', LongType(),True)]\n",
    "ureal = StructType(fields=ureal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MREAL\n",
    "\n",
    "mreal_schema = [StructField('units', ArrayType(ureal),True)]\n",
    "mreal = StructType(fields=mreal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USINT\n",
    "\n",
    "usint_schema = [StructField('val', IntegerType(),True),\n",
    "              StructField('interval', section,True)]\n",
    "usint = StructType(fields=usint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSINT\n",
    "\n",
    "msint_schema = [StructField('units', ArrayType(usint),False)]\n",
    "msint = StructType(fields=msint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USSTRING\n",
    "\n",
    "usstring_schema = [StructField('val', StringType(),True),\n",
    "              StructField('interval', section,True)]\n",
    "usstring = StructType(fields=usstring_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSSTRING\n",
    "\n",
    "msstring_schema = [StructField('units', ArrayType(usstring),False)]\n",
    "msstring = StructType(fields=msstring_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USREAL\n",
    "\n",
    "usreal_schema = [StructField('a', FloatType(),True),\n",
    "                 StructField('b', FloatType(),True),\n",
    "                 StructField('c', FloatType(),True),\n",
    "                 StructField('r', BooleanType(),True),\n",
    "                 StructField('interval', section,True)]\n",
    "usreal = StructType(fields=usreal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSREAL\n",
    "\n",
    "msreal_schema = [StructField('units', ArrayType(usreal),False)]\n",
    "msreal = StructType(fields=msreal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : INTIME\n",
    "\n",
    "intime_schema = [StructField('val', FloatType(),True),\n",
    "                 StructField('t1', IntegerType(),True)]\n",
    "intime = StructType(fields=intime_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : INSPOINT\n",
    "\n",
    "inspoint_schema = [StructField('val', FloatType(),True),\n",
    "                   StructField('sp', spoint,True)]\n",
    "inspoint = StructType(fields=inspoint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- PM2-5: string (nullable = true)\n",
      " |-- PM10: string (nullable = true)\n",
      " |-- PM1-0: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- BC: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lecture Data\n",
    "df = spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(\"VGP-week3-data.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- PM2-5: string (nullable = true)\n",
      " |-- PM10: string (nullable = true)\n",
      " |-- PM1-0: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- BC: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n",
      "+------+--------------+-------------------+----------------+----------------+-----+----+-----+----+----+--------+-----+------------------+\n",
      "|kit_id|participant_id|               time|             lat|             lon|PM2-5|PM10|PM1-0| NO2|  BC|activity|event|               cle|\n",
      "+------+--------------+-------------------+----------------+----------------+-----+----+-----+----+----+--------+-----+------------------+\n",
      "|    80|       9999964|2019-11-14 09:00:00|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:00:10|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:00:20|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:00:30|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:00:40|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:00:50|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:01:00|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:01:10|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:01:20|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:01:30|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:01:40|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:01:50|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:02:00|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:02:10|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:02:20|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:02:30|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:02:40|48.7717766666667|        2.006005| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.931081666668|\n",
      "|    80|       9999964|2019-11-14 09:02:50|48.7717466666667|2.00590833333333| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.930055000001|\n",
      "|    80|       9999964|2019-11-14 09:03:00|48.7717466666667|2.00590833333333| NULL|NULL| NULL|NULL|NULL|    NULL| NULL| 2040.930055000001|\n",
      "|    80|       9999964|2019-11-14 09:03:10|       48.771765|2.00590333333333| NULL|NULL| NULL|NULL|NULL|    NULL| NULL|2040.9306183333335|\n",
      "+------+--------------+-------------------+----------------+----------------+-----+----+-----+----+----+--------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert colonne Clé (profil spatial a notre DF)\n",
    "df.printSchema()\n",
    "df = df.rdd.map(lambda l :(l[0], l[1],l[2],l[3],l[4],l[5],l[6],l[7],l[8],l[9],l[10],l[11],(17+l[3])*31 + l[4])).toDF(StructType([\n",
    "    StructField(\"kit_id\",IntegerType()),\n",
    "    StructField(\"participant_id\",IntegerType()),\n",
    "    StructField(\"time\",StringType()),\n",
    "    StructField(\"lat\",DoubleType()),\n",
    "    StructField(\"lon\",DoubleType()),\n",
    "    StructField(\"PM2-5\",StringType()),\n",
    "    StructField(\"PM10\",StringType()),\n",
    "    StructField(\"PM1-0\",StringType()),\n",
    "    StructField(\"NO2\",StringType()),\n",
    "    StructField(\"BC\",StringType()),\n",
    "    StructField(\"activity\",StringType()),\n",
    "    StructField(\"event\",StringType()),\n",
    "    StructField(\"cle\",DoubleType()),\n",
    "]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On change les string \"NULL\" en null\n",
    "df = df.withColumn((\"BC\"), when(col(\"BC\") == \"NULL\", None).otherwise(col(\"BC\")))\n",
    "df = df.withColumn((\"PM2-5\"), when(col(\"PM2-5\") == \"NULL\", None).otherwise(col(\"PM2-5\")))\n",
    "df = df.withColumn((\"PM10\"),  when(col(\"PM10\") == \"NULL\",  None).otherwise(col(\"PM10\")))\n",
    "df = df.withColumn((\"PM1-0\"), when(col(\"PM1-0\") == \"NULL\", None).otherwise(col(\"PM1-0\")))\n",
    "df = df.withColumn((\"NO2\"),   when(col(\"NO2\") == \"NULL\",   None).otherwise(col(\"NO2\")))\n",
    "df = df.withColumn((\"activity\"), when(col(\"activity\") == \"NULL\", None).otherwise(col(\"activity\")))\n",
    "df = df.withColumn((\"event\"), when(col(\"event\") == \"NULL\", None).otherwise(col(\"event\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------------+----------------+-----+----+-----+----+----+--------+-----+------------------+\n",
      "|kit_id|participant_id|      time|             lat|             lon|PM2-5|PM10|PM1-0| NO2|  BC|activity|event|               cle|\n",
      "+------+--------------+----------+----------------+----------------+-----+----+-----+----+----+--------+-----+------------------+\n",
      "|    80|       9999964|1573718400|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718410|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718420|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718430|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718440|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718450|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718460|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718470|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718480|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718490|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718500|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718510|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718520|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718530|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718540|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718550|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718560|48.7717766666667|        2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718570|48.7717466666667|2.00590833333333| null|null| null|null|null|    null| null| 2040.930055000001|\n",
      "|    80|       9999964|1573718580|48.7717466666667|2.00590833333333| null|null| null|null|null|    null| null| 2040.930055000001|\n",
      "|    80|       9999964|1573718590|       48.771765|2.00590333333333| null|null| null|null|null|    null| null|2040.9306183333335|\n",
      "+------+--------------+----------+----------------+----------------+-----+----+-----+----+----+--------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- PM2-5: string (nullable = true)\n",
      " |-- PM10: string (nullable = true)\n",
      " |-- PM1-0: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- BC: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- cle: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('time',unix_timestamp('time', 'yyyy-MM-dd HH:mm:ss').alias('time'))\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lon: float (nullable = true)\n",
      " |-- PM2-5: float (nullable = true)\n",
      " |-- PM10: float (nullable = true)\n",
      " |-- PM1-0: float (nullable = true)\n",
      " |-- NO2: float (nullable = true)\n",
      " |-- BC: float (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- cle: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Conversion des types PM2.5, BC ... en double\n",
    "#Remarque dans notre fichier de donnée j'ai eliminer le \"+00\" a chaque fois\n",
    "df = df.withColumn(\"PM2-5\",df[\"PM2-5\"].cast(FloatType()))\n",
    "df = df.withColumn(\"PM10\",df[\"PM10\"].cast(FloatType()))\n",
    "df = df.withColumn(\"PM1-0\",df[\"PM1-0\"].cast(FloatType()))\n",
    "df = df.withColumn(\"NO2\",df[\"NO2\"].cast(FloatType()))\n",
    "df = df.withColumn(\"BC\",df[\"BC\"].cast(FloatType()))\n",
    "df = df.withColumn(\"lat\",df[\"lat\"].cast(FloatType()))\n",
    "df = df.withColumn(\"lon\",df[\"lon\"].cast(FloatType()))\n",
    "df = df.withColumn(\"time\",df[\"time\"].cast(LongType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+------------------+\n",
      "|kit_id|participant_id|      time|      lat|      lon|PM2_5|PM10|PM1_0| NO2|  BC|activity|event|               cle|\n",
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+------------------+\n",
      "|    80|       9999964|1573718400| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718410| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718420| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718430| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718440| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718450| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718460| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718470| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718480| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718490| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718500| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718510| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718520| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718530| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718540| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718550| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718560| 48.77178| 2.006005| null|null| null|null|null|    null| null| 2040.931081666668|\n",
      "|    80|       9999964|1573718570|48.771748|2.0059083| null|null| null|null|null|    null| null| 2040.930055000001|\n",
      "|    80|       9999964|1573718580|48.771748|2.0059083| null|null| null|null|null|    null| null| 2040.930055000001|\n",
      "|    80|       9999964|1573718590|48.771767|2.0059032| null|null| null|null|null|    null| null|2040.9306183333335|\n",
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59972"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def change_column_names(columns):\n",
    "    return [c.replace('-', '_') for c in columns]\n",
    "\n",
    "df = df.toDF(*change_column_names(df.columns))\n",
    "df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_id = df.select('participant_id').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "colonnes_names = ['PM2_5','PM10','PM1_0','NO2','BC','activity','event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9999920, 9999955, 9999975, 9999936, 9999930, 9999960, 9999964, 9999962, 999992]\n",
      "['PM2_5', 'PM10', 'PM1_0', 'NO2', 'BC', 'activity', 'event']\n"
     ]
    }
   ],
   "source": [
    "# Affichage de tout les ID des participants \n",
    "print(liste_id)\n",
    "print(colonnes_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------+-----+----------+----------+\n",
      "| _1|  _2|            _3|   _4|        _5|        _6|\n",
      "+---+----+--------------+-----+----------+----------+\n",
      "|0.0| 0.0|           2.0|false|1573805670|1573805680|\n",
      "|0.0| 0.0|           1.0|false|1573805940|1573805950|\n",
      "|0.0| 0.0|           1.0|false|1573807100|1573807110|\n",
      "|0.0| 0.0|           2.0|false|1573810300|1573810310|\n",
      "|0.0| 0.3| -4.72143088E8|false|1573810300|1573810310|\n",
      "|0.0| 0.0|           2.0|false|1573811410|1573811420|\n",
      "|0.0| 0.7|-1.101667985E9|false|1573811410|1573811420|\n",
      "|0.0|-0.3|  4.72143428E8|false|1573811410|1573811420|\n",
      "|0.0| 0.4| -6.29524559E8|false|1573811410|1573811420|\n",
      "|0.0|-0.7| 1.101667996E9|false|1573811410|1573811420|\n",
      "|0.0| 0.0|           9.0|false|1573811410|1573811420|\n",
      "|0.0| 0.0|           3.0|false|1573816080|1573816090|\n",
      "|0.0|-0.1|  1.57381611E8|false|1573816080|1573816090|\n",
      "|0.0|-0.1|  1.57381611E8|false|1573816080|1573816090|\n",
      "|0.0| 1.0|-1.573816077E9|false|1573816080|1573816090|\n",
      "|0.0| 0.1| -1.57381606E8|false|1573816080|1573816090|\n",
      "|0.0| 0.0|           2.0|false|1573816080|1573816090|\n",
      "|0.0| 0.0|           2.0|false|1573816080|1573816090|\n",
      "|0.0| 1.1|-1.731197686E9|false|1573816080|1573816090|\n",
      "|0.0|-1.0| 1.573816093E9|false|1573816080|1573816090|\n",
      "+---+----+--------------+-----+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_MReal(id_particip, colonne):\n",
    "    #On test si le participant et la colonne existent\n",
    "    if (((id_particip in liste_id) or (id_particip==\"*\")) and (colonne in colonnes_names)):\n",
    "        # On enleve les espace\n",
    "        colonne = colonne.strip()\n",
    "        # On recupere le bon participant (en prenant en compte si c'est * ou avec id), la colonne sans les null et on tri par rapport au timestamp \n",
    "        if id_particip == \"*\":\n",
    "            df_one_participant = df.where(df[colonne].isNotNull()).sort(\"time\")\n",
    "        else : \n",
    "            df_one_participant = df.where((df[colonne].isNotNull()) & (df['participant_id'] == id_particip)).sort(\"time\")\n",
    "        \n",
    "        # A partir de la je reprend ma methode\n",
    "    \n",
    "        #Creation d'une nouvelle DF temp pour faire la jointure et regrouper les tuples (1 avec 2 , 3 avec 4, ect...)\n",
    "        temp = df_one_participant\n",
    "        #On renome les colonnes de temp pour qu'il y'ait pas d'ambiguité\n",
    "        temp = temp.withColumnRenamed('kit_id','kit_id_temp')\n",
    "        temp = temp.withColumnRenamed('participant_id','participant_id_temp')\n",
    "        temp = temp.withColumnRenamed('time','time_temp')\n",
    "        temp = temp.withColumnRenamed('lat','lat_temp')\n",
    "        temp = temp.withColumnRenamed('lon','lon_temp')\n",
    "        temp = temp.withColumnRenamed('PM2_5','PM2_5_temp')\n",
    "        temp = temp.withColumnRenamed('PM10','PM10_temp')\n",
    "        temp = temp.withColumnRenamed('PM1_0','PM1_0_temp')\n",
    "        temp = temp.withColumnRenamed('NO2','NO2_temp')\n",
    "        temp = temp.withColumnRenamed('BC','BC_temp')\n",
    "        temp = temp.withColumnRenamed('activity','activity_temp')\n",
    "        temp = temp.withColumnRenamed('event','event_temp')\n",
    "        \n",
    "        #On decremente le time de temp pour pouvoir faire la jointure avec == (car j'ai pas pu le faire avec == time_temp + 10)\n",
    "        temp = temp.withColumn('time_temp', temp.time_temp - 10)\n",
    "        #On fait la jointure\n",
    "        #On fais le rename ici car y'avais un bug de spark \n",
    "        df_one_participant = df_one_participant.withColumnRenamed('time','time')\n",
    "        x = df_one_participant.join(temp, [temp['time_temp'] == df_one_participant['time'] ])\n",
    "        # On decremente le time_temp pour revenir a un etat cohérent\n",
    "        y = x.select(x.lat,x.lon,x['time'],x['time_temp'] + 10, x.lat_temp,x.lon_temp, x['PM2_5'], x['PM2_5_temp'], x['PM10'], x['PM10_temp'], x['PM1_0'], x['PM1_0_temp'], x['NO2'], x['NO2_temp'], x['BC'], x['BC_temp'], x['activity'], x['activity_temp'], x['event'], x['event_temp'])\n",
    "        \n",
    "        #Le map des differents colonnes  \n",
    "        if (colonne == \"PM2_5\"): \n",
    "            rdd_pm25 = y.rdd.map(lambda r :(0.0, ((r[7]-r[6])/(r[3]-r[2])), (((r[6]*r[3])-(r[7]*r[2]))/((r[3]-r[2]))),False,r[2],r[3])).toDF()\n",
    "            #return spark.createDataFrame(rdd_pm25,mreal)\n",
    "            return rdd_pm25\n",
    "        elif (colonne == \"PM10\"): \n",
    "            rdd_pm10 = y.select(y.columns).rdd.map(lambda r : (0.0, ((r[9]-r[8])/(r[3]-r[2])), (((r[8]*r[3])-(r[9]*r[2]))/((r[3]-r[2]))),False,r[2],r[3])).toDF()\n",
    "            #return spark.createDataFrame(rdd_pm10,mreal)\n",
    "            return rdd_pm10\n",
    "        elif (colonne == \"PM1_0\"): \n",
    "            rdd_pm1_0 = y.select(y.columns).rdd.map(lambda r :(0.0, ((r[11]-r[10])/(r[3]-r[2])), (((r[10]*r[3])-(r[11]*r[2]))/((r[3]-r[2]))), False, r[2], r[3])).toDF()\n",
    "            #return spark.createDataFrame(rdd_pm1_0,mreal)\n",
    "            return rdd_pm1_0\n",
    "        elif (colonne == \"NO2\"): \n",
    "            rdd_no2 = y.select(y.columns).rdd.map(lambda r :  (0.0, ((r[13]-r[12])/(r[3]-r[2])), (((r[12]*r[3])-(r[13]*r[2]))/((r[3]-r[2]))), False, r[2], r[3])).toDF()\n",
    "            #return spark.createDataFrame(rdd_no2,mreal)\n",
    "            return rdd_no2\n",
    "        elif (colonne == \"BC\"): \n",
    "            rdd_bc = y.select(y.columns).rdd.map(lambda r :   (0.0, ((r[15]-r[14])/(r[3]-r[2])), (((r[14]*r[3])-(r[15]*r[2]))/((r[3]-r[2]))), False, r[2], r[3])).toDF()\n",
    "            #return spark.createDataFrame(rdd_bc,mreal)\n",
    "            return rdd_bc\n",
    "        elif (colonne == \"activity\"): \n",
    "            rdd_activity = y.select(y.columns).rdd.map(lambda r : (r[16], r[2], r[3])).toDF()\n",
    "            #return spark.createDataFrame(rdd_activity, mstring)\n",
    "            return rdd_activity\n",
    "        elif (colonne == \"event\"): \n",
    "            rdd_event = y.select(y.columns).rdd.map(lambda r : (r[18], r[2], r[3])).toDF()\n",
    "            #return spark.createDataFrame(rdd_event, mstring)\n",
    "            return rdd_event\n",
    "    else : \n",
    "        #print(\"Ce participant n'existe pas || ou la colonne n'existe pas\")\n",
    "        raise ValueError(\"Ce participant n'existe pas || ou la colonne n'existe pas\")\n",
    "\n",
    "    \n",
    "a = get_MReal(\"*\",\"PM2_5\")\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+-----+------------------+------------------+\n",
      "| _1|                 _2|                 _3|   _4|                _5|                _6|\n",
      "+---+-------------------+-------------------+-----+------------------+------------------+\n",
      "|0.0|               -0.0|                2.0|false| 2041.969001666668|2041.9690006666679|\n",
      "|0.0|               -0.0|                1.0|false|2041.9672149999992|2041.9671606666677|\n",
      "|0.0|                0.0|                1.0|false|        2041.96987|2041.9702190000007|\n",
      "|0.0|               -0.0|                2.0|false|2041.9705983333324|2041.9705973333323|\n",
      "|0.0| -92.48982612442346|  188863.5055910349|false|2041.9705983333324|2041.9381623333343|\n",
      "|0.0|               -0.0|                2.0|false|2041.9696883333322|2041.9622856666656|\n",
      "|0.0|  757.3844987731238|-1546554.1889082526|false|2041.9696883333322|2041.9789306666667|\n",
      "|0.0| -177.5252973528344| 362501.96194624814|false|2041.9453866666654|2041.9622856666656|\n",
      "|0.0| 119.24636298146113|-243489.56076677324|false|2041.9453866666654|2041.9789306666667|\n",
      "|0.0| 426.24015588422264|  -870364.322952263|false|2041.9787083333322|2041.9622856666656|\n",
      "|0.0|                0.0|  9.000000006136021|false|2041.9787083333322|2041.9789306666667|\n",
      "|0.0|               -0.0|   2.99999977262635|false|2040.9309416666656|2040.9309406666655|\n",
      "|0.0|-0.9618384194804207| 1966.0457912013524|false|2040.9309416666656|2041.9706173333334|\n",
      "|0.0|-0.9792346758753834| 2001.5503491469985|false|2040.9309416666656|2041.9521473333318|\n",
      "|0.0|   8.08754606867872|-16503.123013720997|false|2040.9309416666656|2042.1674106666665|\n",
      "|0.0| -0.961836569217484| 1966.0420139806374|false|2041.9706183333335|2040.9309406666655|\n",
      "|0.0|               -0.0|                2.0|false|2041.9706183333335|2041.9706173333334|\n",
      "|0.0|               -0.0|                2.0|false|2041.9706183333335|2041.9521473333318|\n",
      "|0.0| 55.896486482474984|-114136.98306528025|false|2041.9706183333335|2042.1674106666665|\n",
      "|0.0|  8.101018622066496|-16530.619556692352|false|2042.1653533333345|2040.9309406666655|\n",
      "+---+-------------------+-------------------+-----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_USReal(id_particip, colonne):\n",
    "    #On test si le participant et la colonne existent\n",
    "    if (((id_particip in liste_id) or (id_particip==\"*\")) and (colonne in colonnes_names)):\n",
    "        # On enleve les espace\n",
    "        colonne = colonne.strip()\n",
    "        # On recupere le bon participant (en prenant en compte si c'est * ou avec id), la colonne sans les null et on tri par rapport au timestamp \n",
    "        if id_particip == \"*\":\n",
    "            df_one_participant = df.where(df[colonne].isNotNull()).sort(\"time\")\n",
    "        else : \n",
    "            df_one_participant = df.where((df[colonne].isNotNull()) & (df['participant_id'] == id_particip)).sort(\"time\")\n",
    "        \n",
    "        # A partir de la je reprend ma methode\n",
    "        #Creation d'une nouvelle DF temp pour faire la jointure et regrouper les tuples (1 avec 2 , 3 avec 4, ect...)\n",
    "        temp = df_one_participant\n",
    "        #On renome les colonnes de temp pour qu'il y'ait pas d'ambiguité\n",
    "        temp = temp.withColumnRenamed('kit_id','kit_id_temp')\n",
    "        temp = temp.withColumnRenamed('participant_id','participant_id_temp')\n",
    "        temp = temp.withColumnRenamed('time','time_temp')\n",
    "        temp = temp.withColumnRenamed('lat','lat_temp')\n",
    "        temp = temp.withColumnRenamed('lon','lon_temp')\n",
    "        temp = temp.withColumnRenamed('PM2_5','PM2_5_temp')\n",
    "        temp = temp.withColumnRenamed('PM10','PM10_temp')\n",
    "        temp = temp.withColumnRenamed('PM1_0','PM1_0_temp')\n",
    "        temp = temp.withColumnRenamed('NO2','NO2_temp')\n",
    "        temp = temp.withColumnRenamed('BC','BC_temp')\n",
    "        temp = temp.withColumnRenamed('activity','activity_temp')\n",
    "        temp = temp.withColumnRenamed('event','event_temp')\n",
    "        temp = temp.withColumnRenamed('cle','cle_temp')\n",
    "        #On decremente le time de temp pour pouvoir faire la jointure avec == (car j'ai pas pu le faire avec == time_temp + 10)\n",
    "        temp = temp.withColumn('time_temp', temp.time_temp - 10)\n",
    "        #On fait la jointure\n",
    "        #On fais le rename ici car y'avais un bug de spark \n",
    "        df_one_participant = df_one_participant.withColumnRenamed('time','time')\n",
    "        x = df_one_participant.join(temp, [temp['time_temp'] == df_one_participant['time'] ])\n",
    "        # On decremente le time_temp pour revenir a un etat cohérent\n",
    "        y = x.select(x.lat,x.lon,x['time'],x['time_temp'] + 10, x.lat_temp,x.lon_temp, x['PM2_5'], x['PM2_5_temp'], x['PM10'], x['PM10_temp'], x['PM1_0'], x['PM1_0_temp'], x['NO2'], x['NO2_temp'], x['BC'], x['BC_temp'], x['activity'], x['activity_temp'], x['event'], x['event_temp'], x['cle'], x['cle_temp'] - 0.000001) #On ajoute le 0.00001 pour ne pas avoir Division/0\n",
    "        #Le map des differents colonnes  \n",
    "        if (colonne == \"PM2_5\"): \n",
    "            rdd_pm25 = y.rdd.map(lambda r :(0.0, ((r[7]-r[6])/(r[21]-r[20])), (((r[6]*r[21])-(r[7]*r[20]))/((r[21]-r[20]))),False,r[20],r[21])).toDF()\n",
    "            #return spark.createDataFrame(rdd_pm25,mreal)\n",
    "            return rdd_pm25\n",
    "        elif (colonne == \"PM10\"): \n",
    "            rdd_pm10 = y.select(y.columns).rdd.map(lambda r : (0.0, ((r[9]-r[8])/(r[21]-r[20])), (((r[8]*r[21])-(r[9]*r[20]))/((r[21]-r[20]))),False,r[20],r[21])).toDF()\n",
    "            #return spark.createDataFrame(rdd_pm10,mreal)\n",
    "            return rdd_pm10\n",
    "        elif (colonne == \"PM1_0\"): \n",
    "            rdd_pm1_0 = y.select(y.columns).rdd.map(lambda r :(0.0, ((r[11]-r[10])/(r[21]-r[20])), (((r[10]*r[21])-(r[11]*r[20]))/((r[21]-r[20]))), False, r[20],r[21])).toDF()\n",
    "            #return spark.createDataFrame(rdd_pm1_0,mreal)\n",
    "            return rdd_pm1_0\n",
    "        elif (colonne == \"NO2\"): \n",
    "            rdd_no2 = y.select(y.columns).rdd.map(lambda r :  (0.0, ((r[13]-r[12])/(r[21]-r[20])), (((r[12]*r[21])-(r[13]*r[20]))/((r[21]-r[20]))), False, r[20],r[21])).toDF()\n",
    "            #return spark.createDataFrame(rdd_no2,mreal)\n",
    "            return rdd_no2\n",
    "        elif (colonne == \"BC\"): \n",
    "            rdd_bc = y.select(y.columns).rdd.map(lambda r :   (0.0, ((r[15]-r[14])/(r[21]-r[20])), (((r[14]*r[21])-(r[15]*r[20]))/((r[21]-r[20]))), False, r[20],r[21])).toDF()\n",
    "            #return spark.createDataFrame(rdd_bc,mreal)\n",
    "            return rdd_bc\n",
    "        elif (colonne == \"activity\"): \n",
    "            rdd_activity = y.select(y.columns).rdd.map(lambda r : (r[16], r[21], r[20])).toDF()\n",
    "            #return spark.createDataFrame(rdd_activity, mstring)\n",
    "            return rdd_activity\n",
    "        elif (colonne == \"event\"): \n",
    "            rdd_event = y.select(y.columns).rdd.map(lambda r : (r[18], r[21], r[20])).toDF()\n",
    "            #return spark.createDataFrame(rdd_event, mstring)\n",
    "            return rdd_event\n",
    "    else : \n",
    "        #print(\"Ce participant n'existe pas || ou la colonne n'existe pas\")\n",
    "        raise ValueError(\"Ce participant n'existe pas || ou la colonne n'existe pas\")\n",
    "\n",
    "    \n",
    "a = get_USReal(\"*\",\"PM2_5\")\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+-----+------------------+------------------+\n",
      "| _1|                 _2|                 _3|   _4|                _5|                _6|\n",
      "+---+-------------------+-------------------+-----+------------------+------------------+\n",
      "|0.0|               -0.0|                2.0|false| 2041.969001666668|2041.9690006666679|\n",
      "|0.0|               -0.0|                1.0|false|2041.9672149999992|2041.9671606666677|\n",
      "|0.0|                0.0|                1.0|false|        2041.96987|2041.9702190000007|\n",
      "|0.0|               -0.0|                2.0|false|2041.9705983333324|2041.9705973333323|\n",
      "|0.0| -92.48982612442346|  188863.5055910349|false|2041.9705983333324|2041.9381623333343|\n",
      "|0.0|               -0.0|                2.0|false|2041.9696883333322|2041.9622856666656|\n",
      "|0.0|  757.3844987731238|-1546554.1889082526|false|2041.9696883333322|2041.9789306666667|\n",
      "|0.0| -177.5252973528344| 362501.96194624814|false|2041.9453866666654|2041.9622856666656|\n",
      "|0.0| 119.24636298146113|-243489.56076677324|false|2041.9453866666654|2041.9789306666667|\n",
      "|0.0| 426.24015588422264|  -870364.322952263|false|2041.9787083333322|2041.9622856666656|\n",
      "|0.0|                0.0|  9.000000006136021|false|2041.9787083333322|2041.9789306666667|\n",
      "|0.0| -0.961836569217484| 1966.0420139806374|false|2041.9706183333335|2040.9309406666655|\n",
      "|0.0|               -0.0|                2.0|false|2041.9706183333335|2041.9706173333334|\n",
      "|0.0|               -0.0|                2.0|false|2041.9706183333335|2041.9521473333318|\n",
      "|0.0|  8.101018622066496|-16530.619556692352|false|2042.1653533333345|2040.9309406666655|\n",
      "|0.0|   56.4867307531235| -115342.2444670974|false|2042.1653533333345|2041.9706173333334|\n",
      "|0.0|  51.59329474716651|-105349.03899697817|false|2042.1653533333345|2041.9521473333318|\n",
      "|0.0|-0.9617872315047928| 1965.9393795120166|false|2041.9686550000013| 2040.928924000001|\n",
      "|0.0|               -0.0|                2.0|false|2041.9686550000013|2041.9685889999998|\n",
      "|0.0|  27.65986929898094| -56448.82728634982|false|2042.1581416666677| 2040.928924000001|\n",
      "+---+-------------------+-------------------+-----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test Cell\n",
    "a.filter((col('_5') >= 2041.9453866666653) & (col('_6')<=2041.9789306666668)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|                 min|\n",
      "+-------+--------------------+\n",
      "|  count|               18404|\n",
      "|   mean|   4.502119104503627|\n",
      "| stddev|    6.30014596874336|\n",
      "|    min|-2.38418579101562...|\n",
      "|    max|                94.0|\n",
      "+-------+--------------------+\n",
      "\n",
      "Seconde :  213.5202076435089\n"
     ]
    }
   ],
   "source": [
    "def MIN_Mreal(Mreal):\n",
    "    if Mreal in colonnes_names : \n",
    "        df_operation = get_MReal(\"*\",Mreal)\n",
    "        df_operation = df_operation.withColumn(\n",
    "            (\"min\"),\n",
    "            when(\n",
    "                col('_2') > 0,\n",
    "                df_operation[1] * df_operation[4] + df_operation[2]\n",
    "            ).otherwise((df_operation[1] * df_operation[5]) + df_operation[2])\n",
    "        )\n",
    "        return df_operation.describe('min').show()\n",
    "t11 = time.time()\n",
    "MIN_Mreal(\"PM2_5\")\n",
    "print (\"Seconde : \", time.time() - t11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              max|\n",
      "+-------+-----------------+\n",
      "|  count|            18404|\n",
      "|   mean|8.201912627650769|\n",
      "| stddev|8.796360304411078|\n",
      "|    min|              0.0|\n",
      "|    max|94.00000190734863|\n",
      "+-------+-----------------+\n",
      "\n",
      "Seconde :  223.07325387001038\n"
     ]
    }
   ],
   "source": [
    "def MAX_Mreal(Mreal):\n",
    "    if Mreal in colonnes_names : \n",
    "        df_operation = get_MReal(\"*\",Mreal)\n",
    "        df_operation = df_operation.withColumn(\n",
    "            (\"max\"),\n",
    "            when(\n",
    "                col('_2') > 0,\n",
    "                df_operation[1] * df_operation[5] + df_operation[2]\n",
    "            ).otherwise((df_operation[1] * df_operation[4]) + df_operation[2])\n",
    "        )\n",
    "        return df_operation.describe('max').show()\n",
    "    \n",
    "t11 = time.time()\n",
    "MAX_Mreal(\"PM2_5\")\n",
    "print (\"Seconde : \", time.time() - t11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983716.7964051273\n",
      "Seconde :  303.19377040863037\n"
     ]
    }
   ],
   "source": [
    "def MEAN_Mreal(Mreal):\n",
    "    if Mreal in colonnes_names : \n",
    "        df_operation = get_MReal(\"*\",Mreal)\n",
    "        df_operation_1 = df_operation.withColumn('mean',(df_operation[0]/3)*abs(pow(df_operation[5],3) - pow(df_operation[4],3)) + (df_operation[1]/3)*abs(pow(df_operation[5],2) - pow(df_operation[4],2)) + df_operation[2]*abs(df_operation[5]-df_operation[4]))\n",
    "        df_operation_2 = df_operation_1.withColumn('interval', abs(df_operation[5] - df_operation[4]))\n",
    "        interval_sum = df_operation_2.select(F.sum('interval')).collect()[0][0]\n",
    "        mean_sum = df_operation_2.select(F.sum('mean')).collect()[0][0]\n",
    "        r = mean_sum/interval_sum\n",
    "        #print(interval_sum.collect()[0][0], mean_sum.collect()[0][0])\n",
    "        #df_operation.printSchema()\n",
    "        return r\n",
    "t11 = time.time()\n",
    "print(MEAN_Mreal(\"PM2_5\"))\n",
    "print (\"Seconde : \", time.time() - t11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Cell\n",
    "x='buffalo'    \n",
    "exec(\"%s = %s\" % (x,'5'))\n",
    "buffalo = a\n",
    "buffalo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atInstant_Mreal(Mreal, t):\n",
    "    print(\"t = \", t)\n",
    "    if Mreal in colonnes_names : \n",
    "        df_operation = get_MReal(\"*\",Mreal)\n",
    "        df_operation.show()\n",
    "        df_operation = df_operation.filter((col('_5') >= t) &(col('_6') <= t))\n",
    "        df_operation.show()\n",
    "        #df_operation.show()\n",
    "        df_operation = df_operation.withColumn('valeur', (df_operation[1]*t) + df_operation[2])\n",
    "        valuee = df_operation.select(df_operation['valeur']).collect()\n",
    "        print(valuee)\n",
    "        value = df_operation.select(df_operation['valeur']).collect()[0][0]\n",
    "        print(value)\n",
    "        d = [(value,t)]\n",
    "        #print(d)\n",
    "        result = spark.createDataFrame(d, intime)\n",
    "        return result.show()\n",
    "atInstant_Mreal(\"PM2_5\", 1573811410)\n",
    "#a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_USReal(\"*\",\"PM2_5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1836.javaToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 235.0 failed 1 times, most recent failure: Lost task 0.0 in stage 235.0 (TID 9010, DESKTOP-QMR01EP, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:126)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 35 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1979)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1967)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1966)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1966)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:946)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:946)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:946)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2145)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2134)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:748)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2095)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2160)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:199)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:79)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:73)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.createShuffledRDD(ShuffleExchangeExec.scala:83)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:98)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:524)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:451)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:495)\r\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:717)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:65)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:65)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:75)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:73)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.createShuffledRDD(ShuffleExchangeExec.scala:83)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:98)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:524)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:451)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:495)\r\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:717)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:510)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:410)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:46)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:717)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:109)\r\n\tat org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:3236)\r\n\tat sun.reflect.GeneratedMethodAccessor106.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:126)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 35 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-3f97d3a49668>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf_operation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mt11\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mMIN_USReal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PM2_5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Seconde : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-95-3f97d3a49668>\u001b[0m in \u001b[0;36mMIN_USReal\u001b[1;34m(USReal)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mMIN_USReal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUSReal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mUSReal\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolonnes_names\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mdf_operation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_USReal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"*\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mUSReal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         df_operation = df_operation.withColumn(\n\u001b[0;32m      5\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[1;34m\"min\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-20b44be3c412>\u001b[0m in \u001b[0;36mget_USReal\u001b[1;34m(id_particip, colonne)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m#Le map des differents colonnes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcolonne\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"PM2_5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mrdd_pm25\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[1;31m#return spark.createDataFrame(rdd_pm25,mreal)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrdd_pm25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mrdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_rdd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjavaToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_rdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_rdd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1836.javaToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 235.0 failed 1 times, most recent failure: Lost task 0.0 in stage 235.0 (TID 9010, DESKTOP-QMR01EP, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:126)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 35 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1979)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1967)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1966)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1966)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:946)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:946)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:946)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2145)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2134)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:748)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2095)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2160)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:199)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:79)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:73)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.createShuffledRDD(ShuffleExchangeExec.scala:83)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:98)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:524)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:451)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:495)\r\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:717)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:65)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:65)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:75)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:73)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.createShuffledRDD(ShuffleExchangeExec.scala:83)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:98)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:524)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:451)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:495)\r\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:717)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:510)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:410)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:46)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:717)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:109)\r\n\tat org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:3236)\r\n\tat sun.reflect.GeneratedMethodAccessor106.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:126)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 35 more\r\n"
     ]
    }
   ],
   "source": [
    "def MIN_USReal(USReal):\n",
    "    if USReal in colonnes_names : \n",
    "        df_operation = get_USReal(\"*\",USReal)\n",
    "        df_operation = df_operation.withColumn(\n",
    "            (\"min\"),\n",
    "            when(\n",
    "                col('_2') < 0,\n",
    "                df_operation[1] * df_operation[5] + df_operation[2]\n",
    "            ).otherwise((df_operation[1] * df_operation[4]) + df_operation[2])\n",
    "        )\n",
    "        return df_operation.describe('min').show()\n",
    "t11 = time.time()\n",
    "MIN_USReal(\"PM2_5\")\n",
    "print (\"Seconde : \", time.time() - t11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAX_USReal(USReal):\n",
    "    if USReal in colonnes_names : \n",
    "        df_operation = get_USReal(\"*\",USReal)\n",
    "        df_operation = df_operation.withColumn(\n",
    "            (\"max\"),\n",
    "            when(\n",
    "                col('_2') > 0,\n",
    "                df_operation[1] * df_operation[5] + df_operation[2]\n",
    "            ).otherwise((df_operation[1] * df_operation[4]) + df_operation[2])\n",
    "        )\n",
    "        return df_operation.describe('max').show()\n",
    "t11 = time.time()\n",
    "MAX_USReal(\"PM2_5\")\n",
    "print (\"Seconde : \", time.time() - t11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "|age|date|name|\n",
      "+---+----+----+\n",
      "| 27| 168|Roni|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "titi=sc.parallelize([\n",
    "    Row(name='Roni',age=27,date=168)]).toDF()\n",
    "titi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "|age|date|name|\n",
      "+---+----+----+\n",
      "| 27| 168|Roni|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titi.filter((col('age')<29) & (col('date')>166)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
