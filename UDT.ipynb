{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, when, to_timestamp, unix_timestamp\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "# conf = SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = SparkContext.getOrCreate()\n",
    "# spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField,StructType, BooleanType, DoubleType,LongType, IntegerType)\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : SPOINT\n",
    "\n",
    "spoint_schema = [StructField('lat', FloatType(),True),\n",
    "                 StructField('lon', FloatType(),True)]\n",
    "spoint = StructType(fields=spoint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : SECTION\n",
    "\n",
    "section_schema = [StructField('lat1', FloatType(),True),\n",
    "                  StructField('lon1', FloatType(),True),\n",
    "                  StructField('lat2', FloatType(),True),\n",
    "                  StructField('lon2', FloatType(),True)]\n",
    "section = StructType(fields=section_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : SLINE\n",
    "\n",
    "sline_schema = [StructField('rints', ArrayType(section),True)]\n",
    "sline = StructType(fields=sline_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USPOINT\n",
    "\n",
    "uspoint_schema = [StructField('t1', LongType(),True),\n",
    "                  StructField('t2', LongType(),True),\n",
    "                  StructField('lat1', FloatType(),True),\n",
    "                  StructField('lon1', FloatType(),True),\n",
    "                  StructField('lat2', FloatType(),True),\n",
    "                  StructField('lon2', FloatType(),True)]\n",
    "uspoint = StructType(fields=uspoint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSPOINT\n",
    "\n",
    "mspoint_schema = [StructField('rints', ArrayType(uspoint),True)]\n",
    "mspoint = StructType(fields=mspoint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : UINT\n",
    "\n",
    "uint_schema = [StructField('val', IntegerType(),True),\n",
    "               StructField('t1', LongType(),True),\n",
    "               StructField('t2', LongType(),True)]\n",
    "uint = StructType(fields=uint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MINT\n",
    "\n",
    "mint_schema = [StructField('units', ArrayType(uint),True)]\n",
    "mint = StructType(fields=mint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USTRING\n",
    "\n",
    "ustring_schema = [StructField('val', StringType(),True),\n",
    "               StructField('t1', LongType(),True),\n",
    "               StructField('t2', LongType(),True)]\n",
    "ustring = StructType(fields=ustring_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSTRING\n",
    "\n",
    "mstring_schema = [StructField('units', ArrayType(ustring),True)]\n",
    "mstring = StructType(fields=mstring_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : UREAL\n",
    "\n",
    "ureal_schema = [StructField('a', FloatType(),True),\n",
    "              StructField('b', FloatType(),True),\n",
    "              StructField('c', FloatType(),True),\n",
    "              StructField('r', BooleanType(),True),\n",
    "              StructField('t1', LongType(),True),\n",
    "              StructField('t2', LongType(),True)]\n",
    "ureal = StructType(fields=ureal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MREAL\n",
    "\n",
    "mreal_schema = [StructField('units', ArrayType(ureal),True)]\n",
    "mreal = StructType(fields=mreal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USINT\n",
    "\n",
    "usint_schema = [StructField('val', IntegerType(),True),\n",
    "              StructField('interval', section,True)]\n",
    "usint = StructType(fields=usint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSINT\n",
    "\n",
    "msint_schema = [StructField('units', ArrayType(usint),False)]\n",
    "msint = StructType(fields=msint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USSTRING\n",
    "\n",
    "usstring_schema = [StructField('val', StringType(),True),\n",
    "              StructField('interval', section,True)]\n",
    "usstring = StructType(fields=usstring_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSSTRING\n",
    "\n",
    "msstring_schema = [StructField('units', ArrayType(usstring),False)]\n",
    "msstring = StructType(fields=msstring_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USREAL\n",
    "\n",
    "usreal_schema = [StructField('a', FloatType(),True),\n",
    "                 StructField('b', FloatType(),True),\n",
    "                 StructField('c', FloatType(),True),\n",
    "                 StructField('r', BooleanType(),True),\n",
    "                 StructField('interval', section,True)]\n",
    "usreal = StructType(fields=usreal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSREAL\n",
    "\n",
    "msreal_schema = [StructField('units', ArrayType(usreal),False)]\n",
    "msreal = StructType(fields=msreal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : INTIME\n",
    "\n",
    "intime_schema = [StructField('val', FloatType(),True),\n",
    "                 StructField('t1', LongType(),True)]\n",
    "intime = StructType(fields=intime_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : INSPOINT\n",
    "\n",
    "inspoint_schema = [StructField('val', FloatType(),True),\n",
    "                   StructField('sp', spoint,True)]\n",
    "inspoint = StructType(fields=inspoint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- PM2-5: string (nullable = true)\n",
      " |-- PM10: string (nullable = true)\n",
      " |-- PM1-0: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- BC: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(\"VGP-week3-data.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On change les string \"NULL\" en null\n",
    "df = df.withColumn((\"BC\"), when(col(\"BC\") == \"NULL\", None).otherwise(col(\"BC\")))\n",
    "df = df.withColumn((\"PM2-5\"), when(col(\"PM2-5\") == \"NULL\", None).otherwise(col(\"PM2-5\")))\n",
    "df = df.withColumn((\"PM10\"),  when(col(\"PM10\") == \"NULL\",  None).otherwise(col(\"PM10\")))\n",
    "df = df.withColumn((\"PM1-0\"), when(col(\"PM1-0\") == \"NULL\", None).otherwise(col(\"PM1-0\")))\n",
    "df = df.withColumn((\"NO2\"),   when(col(\"NO2\") == \"NULL\",   None).otherwise(col(\"NO2\")))\n",
    "df = df.withColumn((\"activity\"), when(col(\"activity\") == \"NULL\", None).otherwise(col(\"activity\")))\n",
    "df = df.withColumn((\"event\"), when(col(\"event\") == \"NULL\", None).otherwise(col(\"event\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------------+----------------+-----+----+-----+----+----+--------+-----+\n",
      "|kit_id|participant_id|      time|             lat|             lon|PM2-5|PM10|PM1-0| NO2|  BC|activity|event|\n",
      "+------+--------------+----------+----------------+----------------+-----+----+-----+----+----+--------+-----+\n",
      "|    80|       9999964|1573718400|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718410|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718420|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718430|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718440|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718450|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718460|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718470|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718480|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718490|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718500|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718510|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718520|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718530|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718540|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718550|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718560|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718570|48.7717466666667|2.00590833333333| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718580|48.7717466666667|2.00590833333333| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718590|       48.771765|2.00590333333333| null|null| null|null|null|    null| null|\n",
      "+------+--------------+----------+----------------+----------------+-----+----+-----+----+----+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- PM2-5: string (nullable = true)\n",
      " |-- PM10: string (nullable = true)\n",
      " |-- PM1-0: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- BC: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('time',unix_timestamp('time', 'yyyy-MM-dd HH:mm:ss').alias('time'))\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lon: float (nullable = true)\n",
      " |-- PM2-5: float (nullable = true)\n",
      " |-- PM10: float (nullable = true)\n",
      " |-- PM1-0: float (nullable = true)\n",
      " |-- NO2: float (nullable = true)\n",
      " |-- BC: float (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Conversion des types PM2.5, BC ... en double\n",
    "#Remarque dans notre fichier de données j'ai eliminer le \"+00\" a chaque fois\n",
    "df = df.withColumn(\"PM2-5\",df[\"PM2-5\"].cast(FloatType()))\n",
    "df = df.withColumn(\"PM10\",df[\"PM10\"].cast(FloatType()))\n",
    "df = df.withColumn(\"PM1-0\",df[\"PM1-0\"].cast(FloatType()))\n",
    "df = df.withColumn(\"NO2\",df[\"NO2\"].cast(FloatType()))\n",
    "df = df.withColumn(\"BC\",df[\"BC\"].cast(FloatType()))\n",
    "df = df.withColumn(\"lat\",df[\"lat\"].cast(FloatType()))\n",
    "df = df.withColumn(\"lon\",df[\"lon\"].cast(FloatType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+\n",
      "|kit_id|participant_id|      time|      lat|      lon|PM2-5|PM10|PM1-0| NO2|  BC|activity|event|\n",
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+\n",
      "|    80|       9999964|1573718400| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718410| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718420| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718430| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718440| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718450| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718460| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718470| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718480| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718490| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718500| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718510| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718520| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718530| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718540| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718550| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718560| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718570|48.771748|2.0059083| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718580|48.771748|2.0059083| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718590|48.771767|2.0059032| null|null| null|null|null|    null| null|\n",
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp = df\n",
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+\n",
      "|kit_id|participant_id|      time|      lat|      lon|PM2_5|PM10|PM1_0| NO2|  BC|activity|event|\n",
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+\n",
      "|    80|       9999964|1573718400| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718410| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718420| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718430| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718440| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718450| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718460| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718470| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718480| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718490| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718500| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718510| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718520| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718530| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718540| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718550| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718560| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718570|48.771748|2.0059083| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718580|48.771748|2.0059083| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718590|48.771767|2.0059032| null|null| null|null|null|    null| null|\n",
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59972"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def change_column_names(columns):\n",
    "    return [c.replace('-', '_') for c in columns]\n",
    "\n",
    "df_temp = df_temp.toDF(*change_column_names(df_temp.columns))\n",
    "df_temp.show()\n",
    "df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9999920,\n",
       " 9999955,\n",
       " 9999975,\n",
       " 9999936,\n",
       " 9999930,\n",
       " 9999960,\n",
       " 9999964,\n",
       " 9999962,\n",
       " 999992]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_id = df_temp.select('participant_id').distinct().rdd.map(lambda r: r[0])\n",
    "liste_id.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_id = [9999964]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-------+-------+-------+-----+----+----------+-------+\n",
      "|id_participant|trip|PM2_5_t|PM_10_t|PM1_0_t|NO2_t|BC_t|activity_t|event_t|\n",
      "+--------------+----+-------+-------+-------+-----+----+----------+-------+\n",
      "+--------------+----+-------+-------+-------+-----+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_type = StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True), StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True)])\n",
    "dataset = spark.createDataFrame(sc.emptyRDD(), data_type)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_one_participant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-f681a81236a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_one_participant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_one_participant' is not defined"
     ]
    }
   ],
   "source": [
    "df_one_participant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant avec id: 9999964\n"
     ]
    }
   ],
   "source": [
    "# New Cell Amir\n",
    "df_temp = df_temp.where(df_temp['PM2_5'].isNotNull() & df_temp['PM1_0'].isNotNull() & df_temp['PM10'].isNotNull() & df_temp['NO2'].isNotNull() & df_temp['BC'].isNotNull() & df_temp['activity'].isNotNull() & df_temp['event'].isNotNull())\n",
    "# new_participant_row = spark.createDataFrame(sc.emptyRDD(), StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True),StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True),StructField('PM2_5_t', mreal,True), StructField('PM_10_s', msreal,True), StructField('PM1_0_s', msreal,True), StructField('NO2_s', msreal,True), StructField('BC_s', msreal,True), StructField('activity_s', msstring,True), StructField('event_s', msstring,True)]))\n",
    "new_participant_row = spark.createDataFrame(sc.emptyRDD(), StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True),StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True)]))    \n",
    "for id_participant in liste_id : \n",
    "    #MOUCHARD 1\n",
    "    print(\"participant avec id: {}\".format(id_participant))\n",
    "    \n",
    "    # Prendre toutes les données d'un seul participant et les trier par le temps\n",
    "    df_one_participant = df_temp.where(col(\"participant_id\") == id_participant)\n",
    "    df_one_participant = df_one_participant.sort(\"time\")\n",
    "    \n",
    "    trip_temp = spark.createDataFrame(sc.emptyRDD(), mspoint)\n",
    "    PM2_5_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    PM10_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    PM1_0_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    NO2_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    BC_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    activity_temp = spark.createDataFrame(sc.emptyRDD(), mstring)\n",
    "    event_temp = spark.createDataFrame(sc.emptyRDD(), mstring)\n",
    "    #df_one_participant.show()\n",
    "    \n",
    "    # A partir de la je reprend ma methode\n",
    "    \n",
    "    #Creation d'une nouvelle DF temp pour faire la jointure et regrouper les tuples (1 avec 2 , 3 avec 4, ect...)\n",
    "    temp = df_one_participant\n",
    "    #On renome les colonnes de temp pour qu'il y'ait pas d'ambiguité\n",
    "    temp = temp.withColumnRenamed('kit_id','kit_id_temp')\n",
    "    temp = temp.withColumnRenamed('participant_id','participant_id_temp')\n",
    "    temp = temp.withColumnRenamed('time','time_temp')\n",
    "    temp = temp.withColumnRenamed('lat','lat_temp')\n",
    "    temp = temp.withColumnRenamed('lon','lon_temp')\n",
    "    temp = temp.withColumnRenamed('PM2_5','PM2_5_temp')\n",
    "    temp = temp.withColumnRenamed('PM10','PM10_temp')\n",
    "    temp = temp.withColumnRenamed('PM1_0','PM1_0_temp')\n",
    "    temp = temp.withColumnRenamed('NO2','NO2_temp')\n",
    "    temp = temp.withColumnRenamed('BC','BC_temp')\n",
    "    temp = temp.withColumnRenamed('activity','activity_temp')\n",
    "    temp = temp.withColumnRenamed('event','event_temp')\n",
    "    \n",
    "    #temp.show()\n",
    "    #On decremente le time de temp pour pouvoir faire la jointure avec == (car j'ai pas pu le faire avec == time_temp + 10)\n",
    "    temp = temp.withColumn('time_temp', temp.time_temp - 10)\n",
    "    #On fait la jointure\n",
    "    #On fais le rename ici car y'avais un bug de spark \n",
    "    df_one_participant = df_one_participant.withColumnRenamed('time','time')\n",
    "    x = df_one_participant.join(temp, [temp['time_temp'] == df_one_participant['time'] ])\n",
    "    # On decremente le time_temp pour revenir a un etat cohérent\n",
    "    y = x.select(x.lat,x.lon,x['time'],x['time_temp'] + 10, x.lat_temp,x.lon_temp, x['PM2_5'], x['PM2_5_temp'], x['PM10'], x['PM10_temp'], x['PM1_0'], x['PM1_0_temp'], x['NO2'], x['NO2_temp'], x['BC'], x['BC_temp'], x['activity'], x['activity_temp'], x['event'], x['event_temp'])\n",
    "    #y.show()\n",
    "    \n",
    "    # Mettre les données dans les df temporaires\n",
    "        \n",
    "PM2_5_df_temp = y.select(y.columns).rdd.map(lambda r :(0.0, ((r[7]-r[6])/(r[3]-r[2])), (((r[6]*r[3])-(r[7]*r[2]))/((r[3]-r[2]))),False,r[2],r[3]),mreal).toDF()\n",
    "PM10_df_temp = y.select(y.columns).rdd.map(lambda r : (0.0, ((r[9]-r[8])/(r[3]-r[2])), (((r[8]*r[3])-(r[9]*r[2]))/((r[3]-r[2]))),False,r[2],r[3]), mreal).toDF()\n",
    "PM1_0_df_temp = y.select(y.columns).rdd.map(lambda r :(0.0, ((r[11]-r[10])/(r[3]-r[2])), (((r[10]*r[3])-(r[11]*r[2]))/((r[3]-r[2]))), False, r[2], r[3]), mreal).toDF()\n",
    "NO2_df_temp = y.select(y.columns).rdd.map(lambda r :  (0.0, ((r[13]-r[12])/(r[3]-r[2])), (((r[12]*r[3])-(r[13]*r[2]))/((r[3]-r[2]))), False, r[2], r[3]), mreal).toDF()\n",
    "BC_df_temp = y.select(y.columns).rdd.map(lambda r :   (0.0, ((r[15]-r[14])/(r[3]-r[2])), (((r[14]*r[3])-(r[15]*r[2]))/((r[3]-r[2]))), False, r[2], r[3]), mreal).toDF()\n",
    "activity_df_temp = y.select(y.columns).rdd.map(lambda r :(r[16], r[2], r[3]), mstring).toDF()\n",
    "event_df_temp = y.select(y.columns).rdd.map(lambda r :(r[18], r[2], r[3]), mstring).toDF()\n",
    "    \n",
    "trip_df_temp = y.select(y.columns).rdd.map(lambda r :(r[2], r[3], r[0], r[1], r[4], r[5]), mspoint).toDF()\n",
    "    \n",
    "#trip_df_temp = trip_temp.union(spark.createDataFrame([[([(time, t_temp, lat, lon, row.lat, row.lon)])]], mspoint))\n",
    "#PM2_5_df_temp.show(2000)\n",
    "# On rename les colonnes car ça a changer \n",
    "\n",
    "PM2_5_df_temp = PM2_5_df_temp.withColumnRenamed('_1','a')\n",
    "PM2_5_df_temp = PM2_5_df_temp.withColumnRenamed('_2','b')\n",
    "PM2_5_df_temp = PM2_5_df_temp.withColumnRenamed('_3','c')\n",
    "PM2_5_df_temp = PM2_5_df_temp.withColumnRenamed('_4','r')\n",
    "PM2_5_df_temp = PM2_5_df_temp.withColumnRenamed('_5','t1')\n",
    "PM2_5_df_temp = PM2_5_df_temp.withColumnRenamed('_6','t2')\n",
    "\n",
    "PM10_df_temp = PM10_df_temp.withColumnRenamed('_1','a')\n",
    "PM10_df_temp = PM10_df_temp.withColumnRenamed('_2','b')\n",
    "PM10_df_temp = PM10_df_temp.withColumnRenamed('_3','c')\n",
    "PM10_df_temp = PM10_df_temp.withColumnRenamed('_4','r')\n",
    "PM10_df_temp = PM10_df_temp.withColumnRenamed('_5','t1')\n",
    "PM10_df_temp = PM10_df_temp.withColumnRenamed('_6','t2')\n",
    "\n",
    "PM1_0_df_temp = PM1_0_df_temp.withColumnRenamed('_1','a')\n",
    "PM1_0_df_temp = PM1_0_df_temp.withColumnRenamed('_2','b')\n",
    "PM1_0_df_temp = PM1_0_df_temp.withColumnRenamed('_3','c')\n",
    "PM1_0_df_temp = PM1_0_df_temp.withColumnRenamed('_4','r')\n",
    "PM1_0_df_temp = PM1_0_df_temp.withColumnRenamed('_5','t1')\n",
    "PM1_0_df_temp = PM1_0_df_temp.withColumnRenamed('_6','t2')\n",
    "\n",
    "NO2_df_temp = NO2_df_temp.withColumnRenamed('_1','a')\n",
    "NO2_df_temp = NO2_df_temp.withColumnRenamed('_2','b')\n",
    "NO2_df_temp = NO2_df_temp.withColumnRenamed('_3','c')\n",
    "NO2_df_temp = NO2_df_temp.withColumnRenamed('_4','r')\n",
    "NO2_df_temp = NO2_df_temp.withColumnRenamed('_5','t1')\n",
    "NO2_df_temp = NO2_df_temp.withColumnRenamed('_6','t2')\n",
    "\n",
    "BC_df_temp = BC_df_temp.withColumnRenamed('_1','a')\n",
    "BC_df_temp = BC_df_temp.withColumnRenamed('_2','b')\n",
    "BC_df_temp = BC_df_temp.withColumnRenamed('_3','c')\n",
    "BC_df_temp = BC_df_temp.withColumnRenamed('_4','r')\n",
    "BC_df_temp = BC_df_temp.withColumnRenamed('_5','t1')\n",
    "BC_df_temp = BC_df_temp.withColumnRenamed('_6','t2')\n",
    "\n",
    "activity_df_temp = activity_df_temp.withColumnRenamed('_1','val')\n",
    "activity_df_temp = activity_df_temp.withColumnRenamed('_2','t1')\n",
    "activity_df_temp = activity_df_temp.withColumnRenamed('_3','t2')\n",
    "\n",
    "event_df_temp = event_df_temp.withColumnRenamed('_1','val')\n",
    "event_df_temp = event_df_temp.withColumnRenamed('_2','t1')\n",
    "event_df_temp = event_df_temp.withColumnRenamed('_3','t2')\n",
    "event_df_temp.show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.where(df_temp['PM2_5'].isNotNull() & df_temp['PM1_0'].isNotNull() & df_temp['PM10'].isNotNull() & df_temp['NO2'].isNotNull() & df_temp['BC'].isNotNull() & df_temp['activity'].isNotNull() & df_temp['event'].isNotNull())\n",
    "# new_participant_row = spark.createDataFrame(sc.emptyRDD(), StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True),StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True),StructField('PM2_5_t', mreal,True), StructField('PM_10_s', msreal,True), StructField('PM1_0_s', msreal,True), StructField('NO2_s', msreal,True), StructField('BC_s', msreal,True), StructField('activity_s', msstring,True), StructField('event_s', msstring,True)]))\n",
    "new_participant_row = spark.createDataFrame(sc.emptyRDD(), StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True),StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True)]))    \n",
    "for id_participant in liste_id:   \n",
    "        \n",
    "    #MOUCHARD 1\n",
    "    print(\"participant avec id: {}\".format(id_participant))\n",
    "    \n",
    "    # Prendre toutes les données d'un seul participant et les trier par le temps\n",
    "    df_one_participant = df_temp.where(col(\"participant_id\") == id_participant)\n",
    "    df_one_participant = df_one_participant.sort(\"time\")\n",
    "    \n",
    "    #initialiser les df temporaires\n",
    "    trip_temp = spark.createDataFrame(sc.emptyRDD(), mspoint)\n",
    "    PM2_5_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    PM10_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    PM1_0_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    NO2_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    BC_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    activity_temp = spark.createDataFrame(sc.emptyRDD(), mstring)\n",
    "    event_temp = spark.createDataFrame(sc.emptyRDD(), mstring)\n",
    "    \n",
    "#     PM2_5_s_temp = spark.createDataFrame(sc.emptyRDD(), msreal)\n",
    "#     PM10_s_temp = spark.createDataFrame(sc.emptyRDD(), msreal)\n",
    "#     PM1_0_s_temp = spark.createDataFrame(sc.emptyRDD(), msreal)\n",
    "#     NO2_s_temp = spark.createDataFrame(sc.emptyRDD(), msreal)\n",
    "#     BC_s_temp = spark.createDataFrame(sc.emptyRDD(), msreal)\n",
    "#     activity_s_temp = spark.createDataFrame(sc.emptyRDD(), msstring)\n",
    "#     event_s_temp = spark.createDataFrame(sc.emptyRDD(), msstring)\n",
    "    \n",
    "    \n",
    "    # Prendre les premieres données\n",
    "    row = df_one_participant.first()\n",
    "    kit_id = row.kit_id\n",
    "    time=row.time+3600*id_participant\n",
    "    lat=row.lat\n",
    "    lon=row.lon\n",
    "    PM2_5=row.PM2_5\n",
    "    PM10=row.PM10\n",
    "    PM1_0=row.PM1_0\n",
    "    NO2=row.NO2\n",
    "    BC=row.BC\n",
    "    activity=row.activity\n",
    "    event=row.event\n",
    "    \n",
    "    # Boucler toutes les données du participant\n",
    "    while (df_one_participant.count() > 0):\n",
    "        row = df_one_participant.first()\n",
    "\n",
    "        if (row.time+3600*id_participant == time):\n",
    "            time = row.time+3600*id_participant-1\n",
    "        t_temp = row.time+3600*id_participant;\n",
    "        \n",
    "#         # Temporel\n",
    "#         dpm2_5 = [(0.0, (row.PM2_5-PM2_5)/(t_temp-time), (PM2_5*t_temp-row.PM2_5*time)/(t_temp-time), False, time, t_temp)]\n",
    "#         df_pm2_5 = spark.createDataFrame(dpm2_5, ureal)\n",
    "#         print(\"ureal pm2_5:\")\n",
    "#         df_pm2_5.show()\n",
    "        \n",
    "#         dpm10 = [(0.0, (row.PM10-PM10)/(t_temp-time), (PM10*t_temp-row.PM10*time)/(t_temp-time), False, time, t_temp)]\n",
    "#         df_pm10 = spark.createDataFrame(dpm10, ureal)\n",
    "        \n",
    "#         dpm1_0 = [(0.0, (row.PM1_0-PM1_0)/(t_temp-time), (PM1_0*t_temp-row.PM1_0*time)/(t_temp-time), False, time, t_temp)]\n",
    "#         df_pm1_0 = spark.createDataFrame(dpm1_0, ureal)\n",
    "        \n",
    "#         dno2 = [(0.0, (row.NO2-NO2)/(t_temp-time), (NO2*t_temp-row.NO2*time)/(t_temp-time), False, time, t_temp)]\n",
    "#         df_no2 = spark.createDataFrame(dno2, ureal)\n",
    "        \n",
    "#         dbc = [(0.0, (row.BC-BC)/(t_temp-time), (BC*t_temp-row.BC*time)/(t_temp-time), False, time, t_temp)]\n",
    "#         df_bc = spark.createDataFrame(dbc, ureal)\n",
    "        \n",
    "#         dactivity = [(activity, time, t_temp)]\n",
    "#         df_activity = spark.createDataFrame(dactivity, ustring)\n",
    "        \n",
    "#         devent = [(row.event, time, t_temp)]\n",
    "#         df_event = spark.createDataFrame(devent, ustring)\n",
    "        \n",
    "#         # Spatial\n",
    "#         dsp = [(time, t_temp, lat, lon, row.lat, row.lon)]\n",
    "#         df_sp = spark.createDataFrame(dsp, uspoint)\n",
    "        \n",
    "#         dspm2_5 = sc.parallelize([0, (row.PM2_5-PM2_5)/(row.lat-lat), (PM2_5*row.lat-row.PM2_5*lat)/(row.lat-lat), 0, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_pm2_5 = spark.createDataFrame(dspm2_5, usreal)\n",
    "#         dspm10 = sc.parallelize([0, (row.PM10-PM10)/(row.lat-lat), (PM10*row.lat-row.PM10*lat)/(row.lat-lat), 0, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_pm10 = spark.createDataFrame(dspm10, usreal)\n",
    "#         dspm1_0 = sc.parallelize([0, (row.PM1_0-PM1_0)/(row.lat-lat), (PM1_0*row.lat-row.PM1_0*lat)/(row.lat-lat), 0, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_pm1_0 = spark.createDataFrame(dspm1_0, usreal)\n",
    "#         dsno2 = sc.parallelize([0, (row.NO2-NO2)/(row.lat-lat), (NO2*row.lat-row.NO2*lat)/(row.lat-lat), 0, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_no2 = spark.createDataFrame(dsno2, usreal)\n",
    "#         dsbc = sc.parallelize([0, (row.BC-BC)/(row.lat-lat), (BC10*row.lat-row.BC10*lat)/(row.lat-lat), 0, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_bc = spark.createDataFrame(dsbc, usreal)\n",
    "#         dsactivity = sc.parallelize([row.activity, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_activity = spark.createDataFrame(dsactivity, usstring)\n",
    "#         dsevent = sc.parallelize([row.event, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_activity = spark.createDataFrame(dsevent, usstring)\n",
    "        \n",
    "        # Mettre les données dans les df temporaires\n",
    "        \n",
    "        PM2_5_temp = PM2_5_temp.union(spark.createDataFrame([[([(0.0, (row.PM2_5-PM2_5)/(t_temp-time), (PM2_5*t_temp-row.PM2_5*time)/(t_temp-time), False, time, t_temp)])]], mreal))\n",
    "        PM10_temp = PM10_temp.union(spark.createDataFrame([[([(0.0, (row.PM10-PM10)/(t_temp-time), (PM10*t_temp-row.PM10*time)/(t_temp-time), False, time, t_temp)])]], mreal))\n",
    "        PM1_0_temp = PM1_0_temp.union(spark.createDataFrame([[([(0.0, (row.PM1_0-PM1_0)/(t_temp-time), (PM1_0*t_temp-row.PM1_0*time)/(t_temp-time), False, time, t_temp)])]], mreal))\n",
    "        NO2_temp = NO2_temp.union(spark.createDataFrame([[([(0.0, (row.NO2-NO2)/(t_temp-time), (NO2*t_temp-row.NO2*time)/(t_temp-time), False, time, t_temp)])]], mreal))\n",
    "        BC_temp = BC_temp.union(spark.createDataFrame([[([(0.0, (row.BC-BC)/(t_temp-time), (BC*t_temp-row.BC*time)/(t_temp-time), False, time, t_temp)])]], mreal))\n",
    "        activity_temp = activity_temp.union(spark.createDataFrame([[([(activity, time, t_temp)])]], mstring))\n",
    "        event_temp = event_temp.union(spark.createDataFrame([[([(row.event, time, t_temp)])]], mstring))\n",
    "        \n",
    "        trip_temp = trip_temp.union(spark.createDataFrame([[([(time, t_temp, lat, lon, row.lat, row.lon)])]], mspoint))\n",
    "#         PM2_5_s_temp = PM2_5_s_temp.union([df_s_pm2_5])\n",
    "#         PM10_s_temp = PM10_s_temp.union([df_s_pm10])\n",
    "#         PM1_0_s_temp = PM1_0_s_temp.union([df_s_pm1_0])\n",
    "#         NO2_s_temp = NO2_s_temp.union([df_s_no2])\n",
    "#         BC_s_temp = BC_s_temp.union([df_s_bc])\n",
    "#         activity_s_temp = activity_s_temp.union([df_s_activity])\n",
    "#         event_s_temp = event_s_temp.union([de_s_event])\n",
    "#         print(\"mreal pm2_5:\")\n",
    "#         PM2_5_temp.show()\n",
    "        \n",
    "        # Décallage des variables\n",
    "        kit_id = row.kit_id\n",
    "        time=row.time+3600*id_participant\n",
    "        lat=row.lat\n",
    "        lon=row.lon\n",
    "        PM2_5=row.PM2_5\n",
    "        PM10=row.PM10\n",
    "        PM1_0=row.PM1_0\n",
    "        NO2=row.NO2\n",
    "        BC=row.BC\n",
    "        activity=row.activity\n",
    "        event=row.event\n",
    "        \n",
    "        # Passer à la ligne suivante\n",
    "        df_one_participant = df_one_participant.where(col(\"time\") > row.time)\n",
    "        \n",
    "    # FIN WHILE\n",
    "    \n",
    "#     d = sc.parallelize([id_participant, trip_temp, PM2_5_temp, PM10_temp, PM1_0_temp, NO2_temp, BC_temp, activity_temp, event_temp, PM2_5_s_temp, PM10_s_temp, PM1_0_s_temp, NO2_s_temp, BC_s_temp, activity_s_temp, event_s_temp ])\n",
    "#     new_participant_row = spark.createDataFrame(d, StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True), StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True),StructField('PM2_5_t', mreal,True), StructField('PM_10_s', msreal,True), StructField('PM1_0_s', msreal,True), StructField('NO2_s', msreal,True), StructField('BC_s', msreal,True), StructField('activity_s', msstring,True), StructField('event_s', msstring,True)]))\n",
    "    d = [(id_participant, trip_temp.rdd.collect()[0], PM2_5_temp.rdd.collect()[0], PM10_temp.rdd.collect()[0], PM1_0_temp.rdd.collect()[0], NO2_temp.rdd.collect()[0], BC_temp.rdd.collect()[0], activity_temp.rdd.collect()[0], event_temp.rdd.collect()[0])]\n",
    "    new_participant_row = spark.createDataFrame(d, StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True), StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True)]))\n",
    "    \n",
    "    dataset = dataset.union(new_participant_row)\n",
    "# FIN DU FOR\n",
    "dataset.show()\n",
    "dataset.toPandas().to_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfffff = spark.createDataFrame([('0.38030685472943737', '0.34728188900913715')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfffff.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfffff.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfffff.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_type = StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True), StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True)])\n",
    "dataset = spark.createDataFrame(sc.emptyRDD(), data_type)\n",
    "dataset.show()\n",
    "\n",
    "trip_temp = spark.createDataFrame(sc.emptyRDD(), mspoint)\n",
    "PM2_5_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "PM10_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "PM1_0_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "NO2_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "BC_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "activity_temp = spark.createDataFrame(sc.emptyRDD(), mstring)\n",
    "event_temp = spark.createDataFrame(sc.emptyRDD(), mstring)\n",
    "\n",
    "PM2_5_temp = PM2_5_temp.union(spark.createDataFrame([[([(0.0, 0.0, 2.0, False, 0, 1)])]], mreal))\n",
    "PM10_temp = PM10_temp.union(spark.createDataFrame([[([(0.0, 0.0, 2.0, False, 0, 1)])]], mreal))\n",
    "PM1_0_temp = PM1_0_temp.union(spark.createDataFrame([[([(0.0, 0.0, 2.0, False, 0, 1)])]], mreal))\n",
    "NO2_temp = NO2_temp.union(spark.createDataFrame([[([(0.0, 0.0, 2.0, False, 0, 1)])]], mreal))\n",
    "BC_temp = BC_temp.union(spark.createDataFrame([[([(0.0, 0.0, 2.0, False, 0, 1)])]], mreal))\n",
    "activity_temp = activity_temp.union(spark.createDataFrame([[([(\"home\", 0, 1)])]], mstring))\n",
    "event_temp = event_temp.union(spark.createDataFrame([[([(\"cooking\", 0, 1)])]], mstring))\n",
    "trip_temp = trip_temp.union(spark.createDataFrame([[([(0, 1, 4.5, 2.0, 4.8, 2.6)])]], mspoint))\n",
    "trip_temp = trip_temp.union(spark.createDataFrame([[([(1, 2, 8.5, 4.0, 8.8, 5.6)])]], mspoint))\n",
    "\n",
    "print(trip_temp.rdd)\n",
    "print (trip_temp.rdd.collect())\n",
    "\n",
    "d = [(id_participant, trip_temp.rdd.collect()[0], PM2_5_temp.collect()[0], PM10_temp.collect()[0], PM1_0_temp.collect()[0], NO2_temp.collect()[0], BC_temp.collect()[0], activity_temp.collect()[0], event_temp.collect()[0])]\n",
    "new_participant_row = spark.createDataFrame(d, StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True), StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True)]))\n",
    "\n",
    "print(\"dataframe new_participant_row\")\n",
    "new_participant_row.show()\n",
    "\n",
    "dataset = dataset.union(new_participant_row)\n",
    "print(\"dataframe dataset\")\n",
    "dataset.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.toPandas().to_csv('mycsv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
