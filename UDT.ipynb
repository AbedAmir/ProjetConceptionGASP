{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, when, to_timestamp, unix_timestamp\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "# conf = SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = SparkContext.getOrCreate()\n",
    "# spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField,StructType, BooleanType, DoubleType,LongType, IntegerType)\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : SPOINT\n",
    "\n",
    "spoint_schema = [StructField('lat', FloatType(),True),\n",
    "                 StructField('lon', FloatType(),True)]\n",
    "spoint = StructType(fields=spoint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : SECTION\n",
    "\n",
    "section_schema = [StructField('lat1', FloatType(),True),\n",
    "                  StructField('lon1', FloatType(),True),\n",
    "                  StructField('lat2', FloatType(),True),\n",
    "                  StructField('lon2', FloatType(),True)]\n",
    "section = StructType(fields=section_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : SLINE\n",
    "\n",
    "sline_schema = [StructField('rints', ArrayType(section),True)]\n",
    "sline = StructType(fields=sline_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USPOINT\n",
    "\n",
    "uspoint_schema = [StructField('t1', LongType(),True),\n",
    "                  StructField('t2', LongType(),True),\n",
    "                  StructField('lat1', FloatType(),True),\n",
    "                  StructField('lon1', FloatType(),True),\n",
    "                  StructField('lat2', FloatType(),True),\n",
    "                  StructField('lon2', FloatType(),True)]\n",
    "uspoint = StructType(fields=uspoint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSPOINT\n",
    "\n",
    "mspoint_schema = [StructField('rints', ArrayType(uspoint),True)]\n",
    "mspoint = StructType(fields=mspoint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : UINT\n",
    "\n",
    "uint_schema = [StructField('val', IntegerType(),True),\n",
    "               StructField('t1', LongType(),True),\n",
    "               StructField('t2', LongType(),True)]\n",
    "uint = StructType(fields=uint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MINT\n",
    "\n",
    "mint_schema = [StructField('units', ArrayType(uint),True)]\n",
    "mint = StructType(fields=mint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USTRING\n",
    "\n",
    "ustring_schema = [StructField('val', StringType(),True),\n",
    "               StructField('t1', LongType(),True),\n",
    "               StructField('t2', LongType(),True)]\n",
    "ustring = StructType(fields=ustring_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSTRING\n",
    "\n",
    "mstring_schema = [StructField('units', ArrayType(ustring),True)]\n",
    "mstring = StructType(fields=mstring_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : UREAL\n",
    "\n",
    "ureal_schema = [StructField('a', FloatType(),True),\n",
    "              StructField('b', FloatType(),True),\n",
    "              StructField('c', FloatType(),True),\n",
    "              StructField('r', BooleanType(),True),\n",
    "              StructField('t1', LongType(),True),\n",
    "              StructField('t2', LongType(),True)]\n",
    "ureal = StructType(fields=ureal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MREAL\n",
    "\n",
    "mreal_schema = [StructField('units', ArrayType(ureal),True)]\n",
    "mreal = StructType(fields=mreal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USINT\n",
    "\n",
    "usint_schema = [StructField('val', IntegerType(),True),\n",
    "              StructField('interval', section,True)]\n",
    "usint = StructType(fields=usint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSINT\n",
    "\n",
    "msint_schema = [StructField('units', ArrayType(usint),False)]\n",
    "msint = StructType(fields=msint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USSTRING\n",
    "\n",
    "usstring_schema = [StructField('val', StringType(),True),\n",
    "              StructField('interval', section,True)]\n",
    "usstring = StructType(fields=usstring_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSSTRING\n",
    "\n",
    "msstring_schema = [StructField('units', ArrayType(usstring),False)]\n",
    "msstring = StructType(fields=msstring_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : USREAL\n",
    "\n",
    "usreal_schema = [StructField('a', FloatType(),True),\n",
    "                 StructField('b', FloatType(),True),\n",
    "                 StructField('c', FloatType(),True),\n",
    "                 StructField('r', BooleanType(),True),\n",
    "                 StructField('interval', section,True)]\n",
    "usreal = StructType(fields=usreal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : MSREAL\n",
    "\n",
    "msreal_schema = [StructField('units', ArrayType(usreal),False)]\n",
    "msreal = StructType(fields=msreal_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : INTIME\n",
    "\n",
    "intime_schema = [StructField('val', FloatType(),True),\n",
    "                 StructField('t1', LongType(),True)]\n",
    "intime = StructType(fields=intime_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du type : INSPOINT\n",
    "\n",
    "inspoint_schema = [StructField('val', FloatType(),True),\n",
    "                   StructField('sp', spoint,True)]\n",
    "inspoint = StructType(fields=inspoint_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- PM2-5: string (nullable = true)\n",
      " |-- PM10: string (nullable = true)\n",
      " |-- PM1-0: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- BC: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(\"VGP-week3-data.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On change les string \"NULL\" en null\n",
    "df = df.withColumn((\"BC\"), when(col(\"BC\") == \"NULL\", None).otherwise(col(\"BC\")))\n",
    "df = df.withColumn((\"PM2-5\"), when(col(\"PM2-5\") == \"NULL\", None).otherwise(col(\"PM2-5\")))\n",
    "df = df.withColumn((\"PM10\"),  when(col(\"PM10\") == \"NULL\",  None).otherwise(col(\"PM10\")))\n",
    "df = df.withColumn((\"PM1-0\"), when(col(\"PM1-0\") == \"NULL\", None).otherwise(col(\"PM1-0\")))\n",
    "df = df.withColumn((\"NO2\"),   when(col(\"NO2\") == \"NULL\",   None).otherwise(col(\"NO2\")))\n",
    "df = df.withColumn((\"activity\"), when(col(\"activity\") == \"NULL\", None).otherwise(col(\"activity\")))\n",
    "df = df.withColumn((\"event\"), when(col(\"event\") == \"NULL\", None).otherwise(col(\"event\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------------+----------------+-----+----+-----+----+----+--------+-----+\n",
      "|kit_id|participant_id|      time|             lat|             lon|PM2-5|PM10|PM1-0| NO2|  BC|activity|event|\n",
      "+------+--------------+----------+----------------+----------------+-----+----+-----+----+----+--------+-----+\n",
      "|    80|       9999964|1573718400|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718410|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718420|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718430|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718440|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718450|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718460|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718470|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718480|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718490|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718500|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718510|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718520|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718530|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718540|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718550|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718560|48.7717766666667|        2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718570|48.7717466666667|2.00590833333333| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718580|48.7717466666667|2.00590833333333| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718590|       48.771765|2.00590333333333| null|null| null|null|null|    null| null|\n",
      "+------+--------------+----------+----------------+----------------+-----+----+-----+----+----+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- PM2-5: string (nullable = true)\n",
      " |-- PM10: string (nullable = true)\n",
      " |-- PM1-0: string (nullable = true)\n",
      " |-- NO2: string (nullable = true)\n",
      " |-- BC: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('time',unix_timestamp('time', 'yyyy-MM-dd HH:mm:ss').alias('time'))\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kit_id: integer (nullable = true)\n",
      " |-- participant_id: integer (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lon: float (nullable = true)\n",
      " |-- PM2-5: float (nullable = true)\n",
      " |-- PM10: float (nullable = true)\n",
      " |-- PM1-0: float (nullable = true)\n",
      " |-- NO2: float (nullable = true)\n",
      " |-- BC: float (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Conversion des types PM2.5, BC ... en double\n",
    "#Remarque dans notre fichier de données j'ai eliminer le \"+00\" a chaque fois\n",
    "df = df.withColumn(\"PM2-5\",df[\"PM2-5\"].cast(FloatType()))\n",
    "df = df.withColumn(\"PM10\",df[\"PM10\"].cast(FloatType()))\n",
    "df = df.withColumn(\"PM1-0\",df[\"PM1-0\"].cast(FloatType()))\n",
    "df = df.withColumn(\"NO2\",df[\"NO2\"].cast(FloatType()))\n",
    "df = df.withColumn(\"BC\",df[\"BC\"].cast(FloatType()))\n",
    "df = df.withColumn(\"lat\",df[\"lat\"].cast(FloatType()))\n",
    "df = df.withColumn(\"lon\",df[\"lon\"].cast(FloatType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+\n",
      "|kit_id|participant_id|      time|      lat|      lon|PM2-5|PM10|PM1-0| NO2|  BC|activity|event|\n",
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+\n",
      "|    80|       9999964|1573718400| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718410| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718420| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718430| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718440| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718450| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718460| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718470| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718480| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718490| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718500| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718510| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718520| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718530| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718540| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718550| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718560| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718570|48.771748|2.0059083| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718580|48.771748|2.0059083| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718590|48.771767|2.0059032| null|null| null|null|null|    null| null|\n",
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp = df\n",
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+\n",
      "|kit_id|participant_id|      time|      lat|      lon|PM2_5|PM10|PM1_0| NO2|  BC|activity|event|\n",
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+\n",
      "|    80|       9999964|1573718400| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718410| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718420| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718430| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718440| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718450| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718460| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718470| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718480| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718490| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718500| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718510| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718520| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718530| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718540| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718550| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718560| 48.77178| 2.006005| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718570|48.771748|2.0059083| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718580|48.771748|2.0059083| null|null| null|null|null|    null| null|\n",
      "|    80|       9999964|1573718590|48.771767|2.0059032| null|null| null|null|null|    null| null|\n",
      "+------+--------------+----------+---------+---------+-----+----+-----+----+----+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59972"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def change_column_names(columns):\n",
    "    return [c.replace('-', '_') for c in columns]\n",
    "\n",
    "df_temp = df_temp.toDF(*change_column_names(df_temp.columns))\n",
    "df_temp.show()\n",
    "df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9999920,\n",
       " 9999955,\n",
       " 9999975,\n",
       " 9999936,\n",
       " 9999930,\n",
       " 9999960,\n",
       " 9999964,\n",
       " 9999962,\n",
       " 999992]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_id = df_temp.select('participant_id').distinct().rdd.map(lambda r: r[0])\n",
    "liste_id.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_id = [9999964]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-------+-------+-------+-----+----+----------+-------+\n",
      "|id_participant|trip|PM2_5_t|PM_10_t|PM1_0_t|NO2_t|BC_t|activity_t|event_t|\n",
      "+--------------+----+-------+-------+-------+-----+----+----------+-------+\n",
      "+--------------+----+-------+-------+-------+-----+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_type = StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True), StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True)])\n",
    "dataset = spark.createDataFrame(sc.emptyRDD(), data_type)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant avec id: 9999964\n"
     ]
    }
   ],
   "source": [
    "#Cell Amir \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_temp = df_temp.where(df_temp['PM2_5'].isNotNull() & df_temp['PM1_0'].isNotNull() & df_temp['PM10'].isNotNull() & df_temp['NO2'].isNotNull() & df_temp['BC'].isNotNull() & df_temp['activity'].isNotNull() & df_temp['event'].isNotNull())\n",
    "# new_participant_row = spark.createDataFrame(sc.emptyRDD(), StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True),StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True),StructField('PM2_5_t', mreal,True), StructField('PM_10_s', msreal,True), StructField('PM1_0_s', msreal,True), StructField('NO2_s', msreal,True), StructField('BC_s', msreal,True), StructField('activity_s', msstring,True), StructField('event_s', msstring,True)]))\n",
    "new_participant_row = spark.createDataFrame(sc.emptyRDD(), StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True),StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True)]))    \n",
    "for id_participant in liste_id:   \n",
    "        \n",
    "    #MOUCHARD 1\n",
    "    print(\"participant avec id: {}\".format(id_participant))\n",
    "    \n",
    "    # Prendre toutes les données d'un seul participant et les trier par le temps\n",
    "    df_one_participant = df_temp.where(col(\"participant_id\") == id_participant)\n",
    "    df_one_participant = df_one_participant.sort(\"time\")\n",
    "#     df_one_participant.show(400000)\n",
    "    #initialiser les df temporaires\n",
    "    trip_temp = spark.createDataFrame(sc.emptyRDD(), mspoint)\n",
    "    PM2_5_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    PM10_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    PM1_0_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    NO2_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    BC_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    activity_temp = spark.createDataFrame(sc.emptyRDD(), mstring)\n",
    "    event_temp = spark.createDataFrame(sc.emptyRDD(), mstring)\n",
    "    # Prendre les premieres données\n",
    "    row = df_one_participant.first()\n",
    "    kit_id = row.kit_id\n",
    "    time=row.time+3600*id_participant\n",
    "    lat=row.lat\n",
    "    lon=row.lon\n",
    "    PM2_5=row.PM2_5\n",
    "    PM10=row.PM10\n",
    "    PM1_0=row.PM1_0\n",
    "    NO2=row.NO2\n",
    "    BC=row.BC\n",
    "    activity=row.activity\n",
    "    event=row.event\n",
    "#     Modif Amir\n",
    "    df_one_participant_panda = df_one_participant.toPandas()\n",
    "    #for row in df_one_participant_panda.index:\n",
    "        #print(df_one_participant_panda['event'][row],df_one_participant_panda['time'][row])\n",
    "        \n",
    "    while (not df_one_participant_panda.empty):\n",
    "        row = df_one_participant.first()\n",
    "\n",
    "        if (row.time+3600*id_participant == time):\n",
    "            time = row.time+3600*id_participant-1\n",
    "        t_temp = row.time+3600*id_participant; \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant avec id: 9999964\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o55840.count.\n: java.lang.OutOfMemoryError: Java heap space\r\n\tat java.util.HashMap.newNode(HashMap.java:1742)\r\n\tat java.util.HashMap.putVal(HashMap.java:641)\r\n\tat java.util.HashMap.putMapEntries(HashMap.java:514)\r\n\tat java.util.HashMap.putAll(HashMap.java:784)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3689)\r\n\tat org.codehaus.janino.UnitCompiler.access$5800(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitLocalVariableDeclarationStatement(UnitCompiler.java:3574)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitLocalVariableDeclarationStatement(UnitCompiler.java:3542)\r\n\tat org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3712)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3541)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3598)\r\n\tat org.codehaus.janino.UnitCompiler.access$4700(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitBlock(UnitCompiler.java:3560)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitBlock(UnitCompiler.java:3542)\r\n\tat org.codehaus.janino.Java$Block.accept(Java.java:2969)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3541)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3604)\r\n\tat org.codehaus.janino.UnitCompiler.access$4800(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitDoStatement(UnitCompiler.java:3561)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitDoStatement(UnitCompiler.java:3542)\r\n\tat org.codehaus.janino.Java$DoStatement.accept(Java.java:3664)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3541)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3598)\r\n\tat org.codehaus.janino.UnitCompiler.access$4700(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitBlock(UnitCompiler.java:3560)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitBlock(UnitCompiler.java:3542)\r\n\tat org.codehaus.janino.Java$Block.accept(Java.java:2969)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3541)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3675)\r\n\tat org.codehaus.janino.UnitCompiler.access$5600(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitWhileStatement(UnitCompiler.java:3569)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitWhileStatement(UnitCompiler.java:3542)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-68115a68a211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m# Boucler toutes les données du participant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf_one_participant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_one_participant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m         \"\"\"\n\u001b[1;32m--> 507\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o55840.count.\n: java.lang.OutOfMemoryError: Java heap space\r\n\tat java.util.HashMap.newNode(HashMap.java:1742)\r\n\tat java.util.HashMap.putVal(HashMap.java:641)\r\n\tat java.util.HashMap.putMapEntries(HashMap.java:514)\r\n\tat java.util.HashMap.putAll(HashMap.java:784)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3689)\r\n\tat org.codehaus.janino.UnitCompiler.access$5800(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitLocalVariableDeclarationStatement(UnitCompiler.java:3574)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitLocalVariableDeclarationStatement(UnitCompiler.java:3542)\r\n\tat org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3712)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3541)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3598)\r\n\tat org.codehaus.janino.UnitCompiler.access$4700(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitBlock(UnitCompiler.java:3560)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitBlock(UnitCompiler.java:3542)\r\n\tat org.codehaus.janino.Java$Block.accept(Java.java:2969)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3541)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3604)\r\n\tat org.codehaus.janino.UnitCompiler.access$4800(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitDoStatement(UnitCompiler.java:3561)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitDoStatement(UnitCompiler.java:3542)\r\n\tat org.codehaus.janino.Java$DoStatement.accept(Java.java:3664)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3541)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3598)\r\n\tat org.codehaus.janino.UnitCompiler.access$4700(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitBlock(UnitCompiler.java:3560)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitBlock(UnitCompiler.java:3542)\r\n\tat org.codehaus.janino.Java$Block.accept(Java.java:2969)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3541)\r\n\tat org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3675)\r\n\tat org.codehaus.janino.UnitCompiler.access$5600(UnitCompiler.java:226)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitWhileStatement(UnitCompiler.java:3569)\r\n\tat org.codehaus.janino.UnitCompiler$12.visitWhileStatement(UnitCompiler.java:3542)\r\n"
     ]
    }
   ],
   "source": [
    "df_temp = df_temp.where(df_temp['PM2_5'].isNotNull() & df_temp['PM1_0'].isNotNull() & df_temp['PM10'].isNotNull() & df_temp['NO2'].isNotNull() & df_temp['BC'].isNotNull() & df_temp['activity'].isNotNull() & df_temp['event'].isNotNull())\n",
    "# new_participant_row = spark.createDataFrame(sc.emptyRDD(), StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True),StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True),StructField('PM2_5_t', mreal,True), StructField('PM_10_s', msreal,True), StructField('PM1_0_s', msreal,True), StructField('NO2_s', msreal,True), StructField('BC_s', msreal,True), StructField('activity_s', msstring,True), StructField('event_s', msstring,True)]))\n",
    "new_participant_row = spark.createDataFrame(sc.emptyRDD(), StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True),StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True)]))    \n",
    "for id_participant in liste_id:   \n",
    "        \n",
    "    #MOUCHARD 1\n",
    "    print(\"participant avec id: {}\".format(id_participant))\n",
    "    \n",
    "    # Prendre toutes les données d'un seul participant et les trier par le temps\n",
    "    df_one_participant = df_temp.where(col(\"participant_id\") == id_participant)\n",
    "    df_one_participant = df_one_participant.sort(\"time\")\n",
    "    \n",
    "    #initialiser les df temporaires\n",
    "    trip_temp = spark.createDataFrame(sc.emptyRDD(), mspoint)\n",
    "    PM2_5_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    PM10_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    PM1_0_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    NO2_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    BC_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "    activity_temp = spark.createDataFrame(sc.emptyRDD(), mstring)\n",
    "    event_temp = spark.createDataFrame(sc.emptyRDD(), mstring)\n",
    "    \n",
    "#     PM2_5_s_temp = spark.createDataFrame(sc.emptyRDD(), msreal)\n",
    "#     PM10_s_temp = spark.createDataFrame(sc.emptyRDD(), msreal)\n",
    "#     PM1_0_s_temp = spark.createDataFrame(sc.emptyRDD(), msreal)\n",
    "#     NO2_s_temp = spark.createDataFrame(sc.emptyRDD(), msreal)\n",
    "#     BC_s_temp = spark.createDataFrame(sc.emptyRDD(), msreal)\n",
    "#     activity_s_temp = spark.createDataFrame(sc.emptyRDD(), msstring)\n",
    "#     event_s_temp = spark.createDataFrame(sc.emptyRDD(), msstring)\n",
    "    \n",
    "    \n",
    "    # Prendre les premieres données\n",
    "    row = df_one_participant.first()\n",
    "    kit_id = row.kit_id\n",
    "    time=row.time+3600*id_participant\n",
    "    lat=row.lat\n",
    "    lon=row.lon\n",
    "    PM2_5=row.PM2_5\n",
    "    PM10=row.PM10\n",
    "    PM1_0=row.PM1_0\n",
    "    NO2=row.NO2\n",
    "    BC=row.BC\n",
    "    activity=row.activity\n",
    "    event=row.event\n",
    "    \n",
    "    # Boucler toutes les données du participant\n",
    "    while (df_one_participant.count() > 0):\n",
    "        row = df_one_participant.first()\n",
    "\n",
    "        if (row.time+3600*id_participant == time):\n",
    "            time = row.time+3600*id_participant-1\n",
    "        t_temp = row.time+3600*id_participant;\n",
    "        \n",
    "#         # Temporel\n",
    "#         dpm2_5 = [(0.0, (row.PM2_5-PM2_5)/(t_temp-time), (PM2_5*t_temp-row.PM2_5*time)/(t_temp-time), False, time, t_temp)]\n",
    "#         df_pm2_5 = spark.createDataFrame(dpm2_5, ureal)\n",
    "#         print(\"ureal pm2_5:\")\n",
    "#         df_pm2_5.show()\n",
    "        \n",
    "#         dpm10 = [(0.0, (row.PM10-PM10)/(t_temp-time), (PM10*t_temp-row.PM10*time)/(t_temp-time), False, time, t_temp)]\n",
    "#         df_pm10 = spark.createDataFrame(dpm10, ureal)\n",
    "        \n",
    "#         dpm1_0 = [(0.0, (row.PM1_0-PM1_0)/(t_temp-time), (PM1_0*t_temp-row.PM1_0*time)/(t_temp-time), False, time, t_temp)]\n",
    "#         df_pm1_0 = spark.createDataFrame(dpm1_0, ureal)\n",
    "        \n",
    "#         dno2 = [(0.0, (row.NO2-NO2)/(t_temp-time), (NO2*t_temp-row.NO2*time)/(t_temp-time), False, time, t_temp)]\n",
    "#         df_no2 = spark.createDataFrame(dno2, ureal)\n",
    "        \n",
    "#         dbc = [(0.0, (row.BC-BC)/(t_temp-time), (BC*t_temp-row.BC*time)/(t_temp-time), False, time, t_temp)]\n",
    "#         df_bc = spark.createDataFrame(dbc, ureal)\n",
    "        \n",
    "#         dactivity = [(activity, time, t_temp)]\n",
    "#         df_activity = spark.createDataFrame(dactivity, ustring)\n",
    "        \n",
    "#         devent = [(row.event, time, t_temp)]\n",
    "#         df_event = spark.createDataFrame(devent, ustring)\n",
    "        \n",
    "#         # Spatial\n",
    "#         dsp = [(time, t_temp, lat, lon, row.lat, row.lon)]\n",
    "#         df_sp = spark.createDataFrame(dsp, uspoint)\n",
    "        \n",
    "#         dspm2_5 = sc.parallelize([0, (row.PM2_5-PM2_5)/(row.lat-lat), (PM2_5*row.lat-row.PM2_5*lat)/(row.lat-lat), 0, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_pm2_5 = spark.createDataFrame(dspm2_5, usreal)\n",
    "#         dspm10 = sc.parallelize([0, (row.PM10-PM10)/(row.lat-lat), (PM10*row.lat-row.PM10*lat)/(row.lat-lat), 0, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_pm10 = spark.createDataFrame(dspm10, usreal)\n",
    "#         dspm1_0 = sc.parallelize([0, (row.PM1_0-PM1_0)/(row.lat-lat), (PM1_0*row.lat-row.PM1_0*lat)/(row.lat-lat), 0, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_pm1_0 = spark.createDataFrame(dspm1_0, usreal)\n",
    "#         dsno2 = sc.parallelize([0, (row.NO2-NO2)/(row.lat-lat), (NO2*row.lat-row.NO2*lat)/(row.lat-lat), 0, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_no2 = spark.createDataFrame(dsno2, usreal)\n",
    "#         dsbc = sc.parallelize([0, (row.BC-BC)/(row.lat-lat), (BC10*row.lat-row.BC10*lat)/(row.lat-lat), 0, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_bc = spark.createDataFrame(dsbc, usreal)\n",
    "#         dsactivity = sc.parallelize([row.activity, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_activity = spark.createDataFrame(dsactivity, usstring)\n",
    "#         dsevent = sc.parallelize([row.event, SECTION(lat, lon, row.lat, row.lon)])\n",
    "#         df_s_activity = spark.createDataFrame(dsevent, usstring)\n",
    "        \n",
    "        # Mettre les données dans les df temporaires\n",
    "        \n",
    "        PM2_5_temp = PM2_5_temp.union(spark.createDataFrame([[([(0.0, (row.PM2_5-PM2_5)/(t_temp-time), (PM2_5*t_temp-row.PM2_5*time)/(t_temp-time), False, time, t_temp)])]], mreal))\n",
    "        PM10_temp = PM10_temp.union(spark.createDataFrame([[([(0.0, (row.PM10-PM10)/(t_temp-time), (PM10*t_temp-row.PM10*time)/(t_temp-time), False, time, t_temp)])]], mreal))\n",
    "        PM1_0_temp = PM1_0_temp.union(spark.createDataFrame([[([(0.0, (row.PM1_0-PM1_0)/(t_temp-time), (PM1_0*t_temp-row.PM1_0*time)/(t_temp-time), False, time, t_temp)])]], mreal))\n",
    "        NO2_temp = NO2_temp.union(spark.createDataFrame([[([(0.0, (row.NO2-NO2)/(t_temp-time), (NO2*t_temp-row.NO2*time)/(t_temp-time), False, time, t_temp)])]], mreal))\n",
    "        BC_temp = BC_temp.union(spark.createDataFrame([[([(0.0, (row.BC-BC)/(t_temp-time), (BC*t_temp-row.BC*time)/(t_temp-time), False, time, t_temp)])]], mreal))\n",
    "        activity_temp = activity_temp.union(spark.createDataFrame([[([(activity, time, t_temp)])]], mstring))\n",
    "        event_temp = event_temp.union(spark.createDataFrame([[([(row.event, time, t_temp)])]], mstring))\n",
    "        \n",
    "        trip_temp = trip_temp.union(spark.createDataFrame([[([(time, t_temp, lat, lon, row.lat, row.lon)])]], mspoint))\n",
    "#         PM2_5_s_temp = PM2_5_s_temp.union([df_s_pm2_5])\n",
    "#         PM10_s_temp = PM10_s_temp.union([df_s_pm10])\n",
    "#         PM1_0_s_temp = PM1_0_s_temp.union([df_s_pm1_0])\n",
    "#         NO2_s_temp = NO2_s_temp.union([df_s_no2])\n",
    "#         BC_s_temp = BC_s_temp.union([df_s_bc])\n",
    "#         activity_s_temp = activity_s_temp.union([df_s_activity])\n",
    "#         event_s_temp = event_s_temp.union([de_s_event])\n",
    "#         print(\"mreal pm2_5:\")\n",
    "#         PM2_5_temp.show()\n",
    "        \n",
    "        # Décallage des variables\n",
    "        kit_id = row.kit_id\n",
    "        time=row.time+3600*id_participant\n",
    "        lat=row.lat\n",
    "        lon=row.lon\n",
    "        PM2_5=row.PM2_5\n",
    "        PM10=row.PM10\n",
    "        PM1_0=row.PM1_0\n",
    "        NO2=row.NO2\n",
    "        BC=row.BC\n",
    "        activity=row.activity\n",
    "        event=row.event\n",
    "        \n",
    "        # Passer à la ligne suivante\n",
    "        df_one_participant = df_one_participant.where(col(\"time\") > row.time)\n",
    "        \n",
    "    # FIN WHILE\n",
    "    \n",
    "#     d = sc.parallelize([id_participant, trip_temp, PM2_5_temp, PM10_temp, PM1_0_temp, NO2_temp, BC_temp, activity_temp, event_temp, PM2_5_s_temp, PM10_s_temp, PM1_0_s_temp, NO2_s_temp, BC_s_temp, activity_s_temp, event_s_temp ])\n",
    "#     new_participant_row = spark.createDataFrame(d, StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True), StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True),StructField('PM2_5_t', mreal,True), StructField('PM_10_s', msreal,True), StructField('PM1_0_s', msreal,True), StructField('NO2_s', msreal,True), StructField('BC_s', msreal,True), StructField('activity_s', msstring,True), StructField('event_s', msstring,True)]))\n",
    "    d = [(id_participant, trip_temp.rdd.collect()[0], PM2_5_temp.rdd.collect()[0], PM10_temp.rdd.collect()[0], PM1_0_temp.rdd.collect()[0], NO2_temp.rdd.collect()[0], BC_temp.rdd.collect()[0], activity_temp.rdd.collect()[0], event_temp.rdd.collect()[0])]\n",
    "    new_participant_row = spark.createDataFrame(d, StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True), StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True)]))\n",
    "    \n",
    "    dataset = dataset.union(new_participant_row)\n",
    "# FIN DU FOR\n",
    "dataset.show()\n",
    "dataset.toPandas().to_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfffff = spark.createDataFrame([('0.38030685472943737', '0.34728188900913715')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfffff.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfffff.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfffff.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_type = StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True), StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True)])\n",
    "dataset = spark.createDataFrame(sc.emptyRDD(), data_type)\n",
    "dataset.show()\n",
    "\n",
    "trip_temp = spark.createDataFrame(sc.emptyRDD(), mspoint)\n",
    "PM2_5_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "PM10_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "PM1_0_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "NO2_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "BC_temp = spark.createDataFrame(sc.emptyRDD(), mreal)\n",
    "activity_temp = spark.createDataFrame(sc.emptyRDD(), mstring)\n",
    "event_temp = spark.createDataFrame(sc.emptyRDD(), mstring)\n",
    "\n",
    "PM2_5_temp = PM2_5_temp.union(spark.createDataFrame([[([(0.0, 0.0, 2.0, False, 0, 1)])]], mreal))\n",
    "PM10_temp = PM10_temp.union(spark.createDataFrame([[([(0.0, 0.0, 2.0, False, 0, 1)])]], mreal))\n",
    "PM1_0_temp = PM1_0_temp.union(spark.createDataFrame([[([(0.0, 0.0, 2.0, False, 0, 1)])]], mreal))\n",
    "NO2_temp = NO2_temp.union(spark.createDataFrame([[([(0.0, 0.0, 2.0, False, 0, 1)])]], mreal))\n",
    "BC_temp = BC_temp.union(spark.createDataFrame([[([(0.0, 0.0, 2.0, False, 0, 1)])]], mreal))\n",
    "activity_temp = activity_temp.union(spark.createDataFrame([[([(\"home\", 0, 1)])]], mstring))\n",
    "event_temp = event_temp.union(spark.createDataFrame([[([(\"cooking\", 0, 1)])]], mstring))\n",
    "trip_temp = trip_temp.union(spark.createDataFrame([[([(0, 1, 4.5, 2.0, 4.8, 2.6)])]], mspoint))\n",
    "trip_temp = trip_temp.union(spark.createDataFrame([[([(1, 2, 8.5, 4.0, 8.8, 5.6)])]], mspoint))\n",
    "\n",
    "print(trip_temp.rdd)\n",
    "print (trip_temp.rdd.collect())\n",
    "\n",
    "d = [(id_participant, trip_temp.rdd.collect()[0], PM2_5_temp.collect()[0], PM10_temp.collect()[0], PM1_0_temp.collect()[0], NO2_temp.collect()[0], BC_temp.collect()[0], activity_temp.collect()[0], event_temp.collect()[0])]\n",
    "new_participant_row = spark.createDataFrame(d, StructType([StructField('id_participant', IntegerType(),True), StructField('trip', mspoint,True), StructField('PM2_5_t', mreal,True), StructField('PM_10_t', mreal,True), StructField('PM1_0_t', mreal,True), StructField('NO2_t', mreal,True), StructField('BC_t', mreal,True), StructField('activity_t', mstring,True), StructField('event_t', mstring,True)]))\n",
    "\n",
    "print(\"dataframe new_participant_row\")\n",
    "new_participant_row.show()\n",
    "\n",
    "dataset = dataset.union(new_participant_row)\n",
    "print(\"dataframe dataset\")\n",
    "dataset.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.toPandas().to_csv('mycsv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
